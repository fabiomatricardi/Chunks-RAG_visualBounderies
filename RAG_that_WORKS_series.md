REF

[https://chat.qwen.ai/s/d31f83c3-c3fd-40da-9f42-f4ed59ece525?fev=0.0.248](https://chat.qwen.ai/s/d31f83c3-c3fd-40da-9f42-f4ed59ece525?fev=0.0.248)


To achieve your goal, weâ€™ll build a **Gradio app** with two tabs:

---

### âœ… Features Summary:

**Tab 1: PDF â†’ Markdown**
- Upload a PDF file.
- Convert it to Markdown using `docling`.
- Save the Markdown file in the current working directory with the same name (`.md` extension).
- Show a success message.

**Tab 2: Markdown Chunking Interface**
- List all `.md` files in the current directory.
- Load and preview the selected Markdown file.
- Allow the user to **manually insert chunk delimiters** (e.g., `<!-- CHUNK -->`) directly in a text editor.
- Provide a **"Process Chunks"** button that:
  - Splits the text on the delimiter.
  - Returns a list of chunks with metadata (`filename`, `chunk_index`, etc.).
  - Shows the chunks in a structured format (e.g., JSON or list).
- Allow **saving** the chunk list as a `.json` file.

---

### ğŸ“¦ Requirements

Install these if not already:

```bash
pip install gradio docling easygui
```

> Note: `easygui` is not needed in Gradio (weâ€™ll use Gradio components instead).

---

### ğŸ§  Design Decisions

- **Chunk delimiter**: Weâ€™ll use a special comment like `<!-- CHUNK -->`, which is Markdown-safe and unlikely to appear in real content.
- **Chunk metadata**: Each chunk will be a dict like:
  ```python
  {
    "filename": "paper.md",
    "chunk_index": 0,
    "text": "..."
  }
  ```
- **Saving chunks**: We'll let the user download the list as JSON.

---

### ğŸ’» Full Gradio App Code
See below after enhanchement


---

### ğŸ“ Output Example (`paper_chunks.json`):

```json
[
  {
    "filename": "paper.md",
    "chunk_index": 0,
    "text": "Introduction...\n..."
  },
  {
    "filename": "paper.md",
    "chunk_index": 1,
    "text": "Related Work...\n..."
  }
]
```

---

### âœ… Next Steps (Optional Enhancements)

- Add **undo/redo** in the editor.
- Support **drag-and-drop chunk markers** (advanced UI).
- Add **chunk validation** (e.g., min/max length).
- Allow **renaming/saving edited Markdown**.

Let me know if you'd like any of these!

# Adding tiktoken KPI

Great! Adding **token count per chunk** using `tiktoken` is a smart enhancementâ€”especially useful for LLM processing (e.g., ensuring chunks fit within context windows).

Weâ€™ll:
1. Use `tiktoken` to count tokens (defaulting to `cl100k_base`, used by GPT-3.5/4).
2. Add a `"token_count"` field to each chunkâ€™s metadata.
3. Show token counts in the chunk preview.
4. Include them in the saved JSON.

---

### ğŸ”§ Step 1: Install `tiktoken`

If not already installed:

```bash
pip install tiktoken
```

---

### âœ… Updated Code with Token Count (KPI)


```python
import os
import json
from pathlib import Path
from docling.document_converter import DocumentConverter
import gradio as gr
import tiktoken

# Constants
CHUNK_DELIMITER = "<!-- CHUNK -->"
OUTPUT_DIR = Path.cwd()
ENCODING_NAME = "cl100k_base"  # Standard for GPT-3.5/4; change if needed

# Initialize tokenizer once
tokenizer = tiktoken.get_encoding(ENCODING_NAME)

def count_tokens(text: str) -> int:
    return len(tokenizer.encode(text))

# Ensure output dir exists
OUTPUT_DIR.mkdir(exist_ok=True)

def list_chunk_files():
    return [f.name for f in OUTPUT_DIR.glob("*_chunks.json")]  

# ----------------------------
# Tab 1: PDF to Markdown
# ----------------------------
def convert_pdf_to_md(pdf_file):
    if pdf_file is None:
        return "âŒ No file uploaded."
    
    try:
        converter = DocumentConverter()
        result = converter.convert(pdf_file.name)
        md_content = result.document.export_to_markdown()

        # Derive output filename
        md_filename = Path(pdf_file.name).with_suffix('.md').name
        md_path = OUTPUT_DIR / md_filename

        with open(md_path, "w", encoding="utf-8") as f:
            f.write(md_content)

        return f"âœ… Successfully converted and saved as `{md_filename}`"
    except Exception as e:
        return f"âŒ Error: {str(e)}"


# ----------------------------
# Tab 2: Markdown Chunking
# ----------------------------
def list_markdown_files():
    return [f.name for f in OUTPUT_DIR.glob("*.md")]

def load_markdown_file(filename):
    if not filename:
        return "", ""
    try:
        with open(OUTPUT_DIR / filename, "r", encoding="utf-8") as f:
            content = f.read()
        return content, content
    except Exception as e:
        return f"Error loading file: {e}", ""

def process_chunks(md_text, filename):
    if not md_text or not filename:
        return "âš ï¸ No content or filename.", None

    raw_chunks = md_text.split(CHUNK_DELIMITER)
    chunks = []
    total_tokens = 0
    preview_lines = []

    for i, chunk_text in enumerate(raw_chunks):
        chunk_text = chunk_text.strip()
        if not chunk_text:
            continue

        token_count = count_tokens(chunk_text)
        total_tokens += token_count

        chunk_meta = {
            "filename": filename,
            "chunk_index": i,
            "token_count": token_count,
            "text": chunk_text
        }
        chunks.append(chunk_meta)

        # Preview line: show first 100 chars + token count
        preview = f"[Chunk {i}] ({token_count} tokens)\n{chunk_text[:100]}{'...' if len(chunk_text) > 100 else ''}"
        preview_lines.append(preview)

    # Build preview display
    display_text = (
        f"âœ… Total chunks: {len(chunks)} | Total tokens: {total_tokens}\n\n"
        + ("\n" + "="*60 + "\n").join(preview_lines)
    )

    # Save as JSON
    json_path = OUTPUT_DIR / f"{Path(filename).stem}_chunks.json"
    with open(json_path, "w", encoding="utf-8") as f:
        json.dump(chunks, f, indent=2, ensure_ascii=False)

    return display_text, str(json_path)

# ----------------------------
# Gradio Interface
# ----------------------------
with gr.Blocks(title="PDF to Markdown & Chunk Editor") as demo:
    gr.Markdown("# ğŸ“„ PDF to Markdown Converter & Chunk Editor")

    with gr.Tabs():
        # ---------------- Tab 1 ----------------
        with gr.Tab("ğŸ“„ Convert PDF to Markdown"):
            pdf_input = gr.File(label="Upload PDF", file_types=[".pdf"])
            convert_btn = gr.Button("Convert to Markdown")
            convert_output = gr.Textbox(label="Result", interactive=False)

            convert_btn.click(
                fn=convert_pdf_to_md,
                inputs=pdf_input,
                outputs=convert_output
            )

        # ---------------- Tab 2 ----------------
        with gr.Tab("âœ‚ï¸ Chunk Editor"):
            with gr.Row():
                md_file_dropdown = gr.Dropdown(
                    choices=list_markdown_files(),
                    label="Select Markdown File",
                    interactive=True
                )
                refresh_btn = gr.Button("ğŸ”„ Refresh List")

            with gr.Row():
                original_md_preview = gr.Markdown(label="Original Preview")
                editable_md = gr.Textbox(
                    label="Editable Markdown (Insert chunk breaks with <!-- CHUNK -->)",
                    lines=20,
                    max_lines=30
                )

            selected_filename = gr.State("")  # to track current file

            def on_file_select(filename):
                content, _ = load_markdown_file(filename)
                return content, content, filename

            md_file_dropdown.change(
                fn=on_file_select,
                inputs=md_file_dropdown,
                outputs=[editable_md, original_md_preview, selected_filename]
            )

            refresh_btn.click(
                fn=lambda: gr.update(choices=list_markdown_files()),
                inputs=None,
                outputs=md_file_dropdown
            )

            process_btn = gr.Button("âœ‚ï¸ Process Chunks")
            chunks_output = gr.Textbox(label="Processed Chunks (Preview)", lines=10)
            download_json = gr.File(label="Download Chunks as JSON", visible=True)

            process_btn.click(
                fn=process_chunks,
                inputs=[editable_md, selected_filename],
                outputs=[chunks_output, download_json]
            )
        # ---------------- Tab 3: Explore Chunk Files ----------------
        with gr.Tab("ğŸ“Š Explore Chunk Files"):
            with gr.Row():       
                chunk_file_dropdown = gr.Dropdown(
                    choices=list_chunk_files(),
                    label="Select Chunk JSON File",
                    interactive=True,
                    allow_custom_value=True
                )
                refresh_chunk_btn = gr.Button("ğŸ”„ Refresh List")

            chunk_json_viewer = gr.JSON(label="Chunks (click to expand)")

            def load_chunks_as_json(filename):
                if isinstance(filename, list):
                    filename = filename[0] if filename else ""
                if not filename or not isinstance(filename, str):
                    return {}
                try:
                    with open(OUTPUT_DIR / filename, "r", encoding="utf-8") as f:
                        return json.load(f)
                except Exception as e:
                    return {"error": str(e)}

            # Remove the demo.load() line!

            refresh_chunk_btn.click(
                fn=list_chunk_files,
                outputs=chunk_file_dropdown
            )

            chunk_file_dropdown.change(
                fn=load_chunks_as_json,
                inputs=chunk_file_dropdown,
                outputs=chunk_json_viewer
            )

# Launch
if __name__ == "__main__":
    demo.launch()
```

---

### ğŸ“ Example JSON Output (with token count)

```json
[
  {
    "filename": "paper.md",
    "chunk_index": 0,
    "token_count": 312,
    "text": "Introduction...\n..."
  },
  {
    "filename": "paper.md",
    "chunk_index": 1,
    "token_count": 287,
    "text": "Related Work...\n..."
  }
]
```

---

### ğŸ–¼ï¸ UI Preview Example

In the Gradio output textbox, youâ€™ll see:

```
âœ… Total chunks: 3 | Total tokens: 842

[Chunk 0] (312 tokens)
Introduction to large language models...

============================================================
[Chunk 1] (287 tokens)
Prior work in document understanding...

...
```

---

### ğŸ’¡ Notes

- **Encoding**: `cl100k_base` works for `gpt-3.5-turbo`, `gpt-4`, etc.  
  If youâ€™re using another model (e.g., `text-embedding-ada-002`), it also uses `cl100k_base`.
- **Performance**: Tokenization is fast, but for huge files, consider async or progress tracking (not needed for typical use).
- **Empty chunks**: We skip empty/space-only chunks automatically.

---

### âœ… Final Integration

Just replace:
- the `import` section (add `tiktoken`),
- define `tokenizer` and `count_tokens`,
- and use the new `process_chunks`.

Everything else in your Gradio app remains unchanged.

Let me know if youâ€™d like to **highlight chunks over a token threshold** (e.g., >512 tokens) or add a **max token slider**!

---

# Analysis

Thanks for sharing the code â€” itâ€™s a clean and well-structured Gradio app that effectively separates concerns across three tabs for **PDF-to-Markdown conversion**, **manual chunk boundary definition**, and **chunk inspection**. The focus on empowering users to **visually design high-quality RAG chunks** is a smart approach, especially as an alternative to LLM-driven chunking (which can be costly and less transparent).

### âœ… Key Strengths:
- **Clear UX flow**: Each tab has a distinct purpose, and the state management (e.g., `selected_filename`) works logically.
- **Token-aware chunking**: Using `tiktoken` with `cl100k_base` aligns with common embedding models (e.g., OpenAI).
- **Non-destructive editing**: Users edit a copy of the Markdown while preserving the original preview.
- **Output traceability**: JSON chunks include source filename and index, aiding debugging.

### âš ï¸ Minor Issues & Suggestions:

#### 1. **`original_md_preview` is a `Markdown` component but assigned raw text**
In Tab 2:
```python
original_md_preview = gr.Markdown(label="Original Preview")
...
return content, content, filename
```
But `gr.Markdown` expects **Markdown-rendered content**, and you're feeding it raw Markdown *as a string*. Thatâ€™s fine â€” Gradio will render it â€” **but note**: if the original Markdown contains `<!-- CHUNK -->`, it will still appear in the preview, which is correct. âœ…

However, the variable name `original_md_preview` suggests it shows the *unedited* source, but if the user reloads the same file after editing, they might expect it to stay pristine. Currently, it **does** stay pristine because you reload from disk on file select â€” so this is actually correct behavior. ğŸ‘

#### 2. **State synchronization risk in Tab 2**
You store `selected_filename` in a `gr.State()`, but if the user edits the text **then changes the dropdown without saving**, the filename in state updates, but the editable text remains. This could lead to accidentally processing the wrong file's content.

â¡ï¸ **Suggestion**: Clear or warn if editing unsaved changes before switching files. But for a lightweight tool, current behavior may be acceptable.

#### 3. **Tab 3: Dropdown choice population at launch**
You removed `demo.load()` (as commented), so on first load, the chunk file dropdown is empty until user clicks "Refresh". Thatâ€™s intentional and fine.

But consider auto-refreshing on tab switch? Gradio doesnâ€™t natively support tab "on focus" events, but you could trigger a refresh via a hidden button with `every=60` or similar â€” probably overkill.

âœ… Your current approach (manual refresh) is simple and reliable.

#### 4. **Security / Path Safety (minor)**
You use `OUTPUT_DIR / filename` directly. If a user uploads a markdown file with `../../../etc/passwd.md` (unlikely in Gradio file uploads, which sanitize names), it could cause issues. But Gradioâ€™s `File` component typically uses safe temporary names, and your dropdown only lists files in `OUTPUT_DIR` with `.md` or `_chunks.json` suffixes.

Still, consider using `Path(filename).name` when constructing paths from dropdowns for extra safety:
```python
safe_filename = Path(filename).name
full_path = OUTPUT_DIR / safe_filename
```

#### 5. **UX Enhancement: Show token count live in Tab 2**
Consider adding a small live token counter below the editable box to help users stay within limits (e.g., 512 or 8192 tokens). Could be done with a `gr.Textbox(visible=False)` and a `.change()` event.

But again â€” optional; current preview after processing is sufficient for many use cases.

---

### ğŸŸ¢ Overall Assessment:
This is a **production-ready prototype** for human-in-the-loop RAG chunk design. It encourages **intentional chunking** â€” a critical but often overlooked step â€” and avoids over-reliance on black-box LLM preprocessing.

Iâ€™m ready for your next instructions! For example:
- Add chunk validation (e.g., max token limit warning)?
- Export to vector DB format (e.g., JSONL for ingestion)?
- Compare two chunking strategies side-by-side?
- Add semantic coherence scoring (via small embedding model)?

Just say the word.


---


I want to write a substack series of posts (2000 words each) on this baseline.
in post one I want to introduce the idea and main core of the project: manually prepare the documents so that it will be ready for effective retrieval. After giving a raw overview of the existing frameworks and strategies I want to shift back the focus on the data preparation.
In post 2 I want to present tab 1 (first step in the process): from pdf to structured markdown (easy to delimit chunk boudaries).
In post 3 I want to present tab 2: the user is domain aware of the content, so he/she is the best suited candidate to set the chunks limits.
In post 4 I want to present tab 3: you can review your data

at the core gradio tabs allow to create modular apps, building the enhancements one after the others. Every app is already working, from post 2 you can already have a practical working application.

I want to expand, then, from post 5 to bm25. The idea that keyword search is still a valid strategy and how to use it. The practical code is to expand the gradio app with another tab (tab 4) where a user can select more than one json file, ask a query, and get the best "k" results with the score.

Post 6 will focus on semantic search, using llama.cpp server to host the embedding model through standard OpenAI compatible API. The semantic search will be practically used in tab 5, where the user can select more than one json file, ask a query, and get the best "k" results with the score.

Post 7 is the final chapter. The gradio app will have a tab 6 where a combined score to a query is given using hybrid search with both bm25 and semantic similarity. the user can select more than one json file, ask a query, and get the best "k" results with the best combined score.

do you have any suggestions?


Your Substack series plan is **excellent**â€”clearly structured, pedagogically sound, and grounded in real engineering pragmatism. Youâ€™re framing a critical but under-discussed truth in the RAG community: **the quality of retrieval starts long before the query hits the vector indexâ€”it starts with how you prepare your data**.

Below are **suggestions to strengthen each post**, plus **strategic framing advice**, **technical enhancements**, and **narrative flow tips** to maximize impact and reader engagement.

---

### ğŸ¯ **Overarching Narrative Suggestion**
**Title the series**:  
> **â€œRAG That Works: Why Chunking Is a Human Jobâ€**  
> or  
> **â€œThe Hidden Labor of RAG: Getting Chunks Rightâ€**

Position yourself not just as a tool-builder, but as an advocate for **intentional, transparent, and efficient RAG design**â€”a counterpoint to the â€œthrow LLMs at everythingâ€ trend.

---

## ğŸ”¹ **Post 1: The Case for Manual Chunk Preparation**
**Core message**: *Great retrieval starts with great chunksâ€”and humans (not LLMs) are best at defining semantic boundaries.*

### âœ… Do:
- **Critique current frameworks**:
  - *GraphRAG*: powerful but requires LLM-heavy graph construction (expensive, opaque).
  - *LightRAG*: relies on LLMs to summarize/relate chunks (still costly).
  - Default langchain splitters (recursive, character): **context-blind**.
- **Introduce your thesis**:  
  > â€œIf your data is pre-chunked with care, even BM25 outperforms lazy semantic search.â€
- Use a **real-world analogy**:  
  > â€œChunking is like indexing a book. You wouldnâ€™t let a toddler draw the chapter breaksâ€”why let a stochastic parrot do it?â€
- Mention **cost + reproducibility**: manual chunking is cheaper, auditable, and deterministic.

### âš ï¸ Avoid:
- Over-promising (â€œmanual always beats LLMâ€). Instead: â€œmanual gives you control; automation gives you speedâ€”choose wisely.â€

---

## ğŸ”¹ **Post 2: From PDF to Structured Markdown (Tab 1)**
**Core message**: *Raw PDFs are RAG poison. Markdown is your detox.*

### âœ… Enhance:
- Show **before/after examples**:
  - PDF table â†’ garbled text vs. clean Markdown table.
  - Docling vs. PyPDF2 vs. pdfplumber (brief comparison).
- Explain **why Markdown?**
  - Human-readable + editable.
  - Delimiters like `<!-- CHUNK -->` donâ€™t interfere with rendering.
  - Preserves structure (headers, lists, code blocks) â†’ better semantic boundaries.
- **Call it â€œThe RAG Readiness Layerâ€**â€”a preprocessing step that makes downstream tasks trivial.

### ğŸ’¡ Pro tip:
> â€œYouâ€™re not converting PDFsâ€”youâ€™re *liberating* documents from layout tyranny.â€

---

## ğŸ”¹ **Post 3: You Are the Best Chunker (Tab 2)**
**Core message**: *Domain experts > LLMs for boundary decisions.*

### âœ… Key arguments:
- LLMs donâ€™t know your **business logic**: Is a financial footnote part of the clause above? Only *you* know.
- Show **bad auto-chunk vs. good manual chunk**:
  - Auto: cuts in the middle of a legal definition.
  - Manual: keeps definition + examples together.
- Emphasize **iterative refinement**: edit â†’ process â†’ inspect (your Tab 3 preview).
- Mention **token efficiency**: avoid redundant context by precise trimming.

### ğŸ’¡ Powerful line:
> â€œAn LLM guesses where your document breathes. You *know*.â€

---

## ğŸ”¹ **Post 4: Inspect, Donâ€™t Assume (Tab 3)**
**Core message**: *RAG pipelines fail in silence. Make your chunks visible.*

### âœ… Do:
- Frame this as **data observability for RAG**.
- Show how users can:
  - Spot oversized chunks (>1024 tokens?).
  - Find empty or near-empty chunks.
  - Verify metadata (source filename, index).
- Argue: **â€œIf you canâ€™t inspect it, you canâ€™t trust it.â€**

### ğŸ’¡ Bonus:
Add a **â€œchunk health scoreâ€** in future (e.g., tokens within ideal range â†’ green, too big â†’ red).

---

## ğŸ”¹ **Post 5: BM25 Is Not Deadâ€”Itâ€™s Underrated (Tab 4)**
**Core message**: *Keyword search is fast, free, and shockingly effective with clean chunks.*

### âœ… Technical plan:
- Use `rank_bm25` (lightweight, pure Python).
- Let user select **multiple `_chunks.json` files** â†’ flatten into a single retrievable corpus.
- Input: query + top-k â†’ output: chunk text + BM25 score.
- Show **side-by-side**: BM25 vs. naive semantic (later).

### ğŸ“ Narrative angle:
> â€œBM25 doesnâ€™t need GPUs. It needs clean dataâ€”and you just built that.â€

### ğŸ’¡ Pro insight:
> â€œHybrid search starts with *two good baselines*, not one fancy model.â€

---

## ğŸ”¹ **Post 6: Semantic Search on Your Laptop (Tab 5)**
**Core message**: *You donâ€™t need OpenAI to embedâ€”run open models locally.*

### âœ… Implementation tips:
- Use `llama.cpp` + `embedding` model (e.g., `nomic-embed-text`, `bge-m3`, or `all-MiniLM-L6-v2` via `llama-box` or `llm-rs`).
- **Gradio + OpenAI-compatible API**: point to `http://localhost:8080/v1` (llama.cpp server).
- Cache embeddings per chunk to avoid recomputation (store in JSON or separate `.npy`).
- Normalize vectors for cosine similarity.

### ğŸ“ Key message:
> â€œPrivacy, cost, and control: your embeddings, your rules.â€

---

## ğŸ”¹ **Post 7: Hybrid Search â€” The Best of Both Worlds (Tab 6)**
**Core message**: *BM25 + semantic = robustness + relevance.*

### âœ… How to combine:
- **Normalize scores**:  
  `bm25_norm = (bm25 - min) / (max - min)`  
  `semantic_norm = cosine_sim` (already [0,1])
- **Weighted sum**: `final_score = Î± * bm25_norm + (1-Î±) * semantic_norm`  
  (Add Î± as a slider in Gradio!)
- Let users **tune Î±** and see results change in real time.

### ğŸ’¡ Killer demo:
Ask: *â€œclimate policy in EUâ€*  
- BM25 finds exact phrase matches.  
- Semantic finds â€œenvironmental regulations in Europe.â€  
- Hybrid finds **both**, ranked optimally.

> â€œHybrid isnâ€™t just betterâ€”itâ€™s *more honest* about uncertainty.â€

---

## ğŸ› ï¸ **Gradio App Evolution Plan**
| Tab | Purpose | Dependencies |
|-----|--------|--------------|
| 1 | PDF â†’ Markdown | `docling` |
| 2 | Manual chunking | `tiktoken` |
| 3 | Inspect chunks | â€” |
| 4 | BM25 search | `rank_bm25` |
| 5 | Semantic search | `llama.cpp` + requests |
| 6 | Hybrid search | both + score fusion |

> âœ… **Modular design**: Each tab is usable standalone. Reader can follow along incrementally.

---

## ğŸ“ˆ **Engagement Boosters**
- **End each post with a challenge**:  
  > â€œTry chunking your own document. Where did your instinct beat the algorithm?â€
- **Share real chunking mistakes** (yours or communityâ€™s).
- **Include Gradio screenshots or short Loom videos**.
- **Offer the full app on GitHub** with a `README.md` that mirrors your posts.

---

## Final Thought
Youâ€™re not just teaching a toolâ€”youâ€™re **advocating for a mindset**:  
> **RAG success is 10% model, 90% data craftsmanship.**

This series could become the **definitive guide to intentional RAG**. Lean into that.

Let me know when you want help drafting any of the postsâ€”I can generate full outlines or even draft sections!


# DRAFT OF THE OUTLINES
Absolutely! Below are **seven detailed, publication-ready outlines** for your Substack series. Each outline is structured for a ~2000-word post, includes key arguments, narrative flow, technical depth, real-world analogies, and Gradio integration notes. They build logically toward your final vision of a **human-centered, modular, hybrid RAG toolkit**.

---

## **Series Title Suggestion**:  
> **"RAG That Works: Why Chunking Is a Human Job"**  
> *A 7-part guide to building retrieval systems that start with thoughtful dataâ€”not just bigger models.*

---

### **Post 1: The Forgotten Foundation of RAG**  
**Subtitle**: *Why your chunks matter more than your embedding model*

#### **1. Hook: The RAG Illusion**
- Story: â€œI spent $200 on LLM calls to â€˜optimizeâ€™ retrievalâ€¦ only to realize my PDFs were chunked mid-sentence.â€
- The myth: â€œBetter models â†’ better RAG.â€ Reality: Garbage chunks in, garbage answers out.

#### **2. The State of RAG Frameworks (and Their Blind Spots)**
- **LangChain default splitters**: recursive, token-basedâ€”context-agnostic.
- **GraphRAG**: powerful but requires LLM-heavy graph construction ($$$, slow, opaque).
- **LightRAG**: uses LLMs to summarize relational contextâ€”still expensive and non-deterministic.
- **Auto-chunking trend**: treats documents as bags of tokens, not structured narratives.

#### **3. The Core Thesis: Data Preparation Is the Bottleneck**
- RAG = **Retrieval + Augmentation + Generation**  
  â†’ But retrieval fails if **data isnâ€™t retrieval-ready**.
- **Humans > LLMs for boundary decisions**:  
  - You know when a legal clause ends.  
  - You know that a footnote belongs with the paragraph above.  
  - An LLM guesses. You *know*.

#### **4. Introducing Our Alternative: Intentional Chunking**
- Goal: **Shift the intelligence upstream**â€”into the hands of domain experts.
- Principles:  
  - Transparent (you see every chunk).  
  - Auditable (reproducible, no black-box LLMs).  
  - Efficient (no wasted tokens, no wasted $).

#### **5. Preview of the Series**
- A 7-step journey:  
  1. Convert PDFs â†’ clean Markdown  
  2. Manually set chunk boundaries  
  3. Inspect your chunks  
  4. Retrieve with BM25 (yes, itâ€™s still great!)  
  5. Add semantic search (locally, privately)  
  6. Combine both for hybrid superpowers  
- Tool: A **modular Gradio app**â€”each tab is a working tool on its own.

#### **6. Closing Thought**
> â€œGreat RAG doesnâ€™t start with a promptâ€”it starts with a paragraph break.â€

---

### **Post 2: From PDF Chaos to Markdown Clarity**  
**Subtitle**: *Tab 1: Why your RAG pipeline begins with structured text*

#### **1. The PDF Problem**
- PDFs are **layout artifacts**, not data formats.  
  - Tables become garbled text.  
  - Columns scramble reading order.  
  - Headers/footers pollute content.
- Show side-by-side: raw PDF extraction vs. Docling output.

#### **2. Why Markdown? The Ideal RAG Intermediate Format**
- Human-readable + machine-parsable.  
- Preserves hierarchy: `#`, `##`, lists, code blocks.  
- Supports **non-rendering comments** like `<!-- CHUNK -->`â€”perfect for delimiters.
- Unlike HTML or XML, itâ€™s **editable in any text editor**.

#### **3. Introducing Tab 1: PDF â†’ Markdown Converter**
- Built with **Docling** (high-fidelity, open-source).  
- One-click conversion.  
- Output saved alongside inputâ€”no cloud dependency.
- Gradio UI: upload â†’ convert â†’ success message.

#### **4. What Makes This â€œRAG-Readyâ€?**
- Clean text = reliable tokenization.  
- Structure = intuitive chunk boundaries (e.g., break after `##` sections).  
- No hidden artifacts = no retrieval noise.

#### **5. Real-World Impact**
- Example: Financial report â†’ tables become Markdown tables â†’ chunks preserve row/column logic.  
- Contrast with PyPDF2: â€œTotal Revenue: $1.2Bâ€ becomes â€œT otal Rev enue: $1 .2Bâ€.

#### **6. Try It Yourself**
- Code snippet: `DocumentConverter().convert(...)`  
- GitHub link to Tab 1 standalone app.

#### **7. Closing**
> â€œYouâ€™re not converting documentsâ€”youâ€™re preparing them for conversation.â€

---

### **Post 3: You Are the Best Chunker**  
**Subtitle**: *Tab 2: Why domain expertsâ€”not algorithmsâ€”should draw the lines*

#### **1. The Chunking Dilemma**
- Auto-splitters cut at 512 tokensâ€”regardless of meaning.  
  - Result: Half a definition + half an example = useless.
- LLM-based chunking: expensive, inconsistent, unexplainable.

#### **2. The Human Advantage**
- You understand:  
  - Narrative flow (â€œthis case study supports the prior claimâ€)  
  - Domain semantics (â€œthis equation belongs with its derivationâ€)  
  - Business logic (â€œthis disclaimer applies to the entire sectionâ€)

#### **3. Introducing Tab 2: The Chunk Editor**
- Load Markdown file â†’ edit in textbox.  
- Insert `<!-- CHUNK -->` anywhere to mark boundaries.  
- Real-time preview of original vs. editable.
- Process â†’ see token counts, chunk previews, and save as structured JSON.

#### **4. Design Philosophy**
- **Non-destructive**: original file untouched.  
- **Token-aware**: shows tokens per chunk (with `tiktoken`).  
- **Structured output**: each chunk = `{text, token_count, filename, index}`.

#### **5. A Chunking Case Study**
- Before: auto-split breaks a product spec mid-feature list.  
- After: manual chunk keeps feature + compatibility notes together.  
- Result: query â€œDoes it support USB-C?â€ â†’ full answer retrieved.

#### **6. Why This Scales**
- Once chunked, reuse across:  
  - BM25  
  - Semantic search  
  - Hybrid systems  
  - Fine-tuning datasets

#### **7. Closing**
> â€œAn LLM sees tokens. You see meaning. Trust your judgment.â€

---

### **Post 4: Inspect, Donâ€™t Assume**  
**Subtitle**: *Tab 3: Making your chunks visibleâ€”and trustworthy*

#### **1. The Silent Failure of RAG**
- Most RAG systems fail **quietly**: no error, just bad answers.  
- Root cause: chunks that look fine but are **semantically broken**.

#### **2. Enter Tab 3: The Chunk Inspector**
- Dropdown: select any `_chunks.json` file.  
- Viewer: expandable JSON showing every chunkâ€™s text, tokens, source.  
- No more â€œblack boxâ€ data.

#### **3. What to Look For**
- **Oversized chunks** (>1024 tokens): risk truncation in LLM context.  
- **Tiny chunks**: likely noise (e.g., page numbers, headers).  
- **Boundary errors**: chunks ending with â€œcontâ€¦â€ or starting mid-sentence.  
- **Token imbalance**: some chunks 50 tokens, others 2000 â†’ inefficient retrieval.

#### **4. Iterative Refinement Loop**
1. Edit in Tab 2  
2. Process  
3. Inspect in Tab 3  
4. Repeat  
- This loop is **your RAG quality control**.

#### **5. Beyond Debugging: Data Documentation**
- Your chunk JSON becomes **a versioned, auditable dataset**.  
- Share with teammates: â€œHereâ€™s how we chunked the compliance manual.â€  
- Track changes over time.

#### **6. Future Vision: Chunk Health Dashboard**
- (Tease Post 5+) â€œWhat if we scored chunks on coherence, length, and redundancy?â€

#### **7. Closing**
> â€œIf you canâ€™t see your data, you donâ€™t own your RAG system.â€

---

### **Post 5: BM25 Is Not Deadâ€”Itâ€™s Underrated**  
**Subtitle**: *Tab 4: Fast, free, and shockingly effective keyword search*

#### **1. The BM25 Comeback**
- Dismissed as â€œold techâ€â€”but powers Google, ElasticSearch, and more.  
- Advantages:  
  - No GPUs  
  - No API costs  
  - Instant indexing  
  - Explainable scores

#### **2. Why BM25 Shines with Clean Chunks**
- Garbage-in: BM25 fails on noisy text.  
- **Your clean Markdown chunks? Perfect input.**  
- Example: query â€œclimate risk disclosureâ€ â†’ finds exact phrase in financial reports.

#### **3. Introducing Tab 4: BM25 Search**
- User selects **multiple `_chunks.json` files** â†’ corpus built on-the-fly.  
- Enter query + top-k â†’ get ranked results with **BM25 scores**.  
- Backend: `rank_bm25` (pure Python, <100 lines).

#### **4. Code Walkthrough**
- Flatten chunks â†’ tokenize (simple whitespace) â†’ build BM25 corpus.  
- Score = term frequency + inverse doc frequency.  
- No magicâ€”just math you can understand.

#### **5. When BM25 Wins**
- Exact phrase matching  
- Named entities (laws, products, codes)  
- Low-resource environments  
- Queries with rare keywords

#### **6. Limitations (and Why Weâ€™ll Add Semantic Next)**
- Fails on synonyms (â€œcarâ€ vs. â€œautomobileâ€)  
- No semantic understanding  
- â†’ But **not a weaknessâ€”just a tool with a scope**

#### **7. Closing**
> â€œBM25 doesnâ€™t need to be smart. It just needs clean dataâ€”and youâ€™ve already built that.â€

---

### **Post 6: Semantic Search on Your Laptop**  
**Subtitle**: *Tab 5: Private, local embeddings with llama.cpp*

#### **1. The Embedding Dilemma**
- Cloud APIs = cost + privacy risks + latency.  
- Open-source models = freedom, but â€œtoo hard to run.â€

#### **2. Enter llama.cpp: Your Local Embedding Server**
- Run `nomic-embed-text` or `bge-m3` via `llama-box` or `llm-rs`.  
- Exposes **OpenAI-compatible `/v1/embeddings` endpoint** on `localhost:8080`.  
- No internet required after download.

#### **3. Introducing Tab 5: Semantic Search**
- Select multiple chunk files â†’ compute embeddings (cached!).  
- Query â†’ embed â†’ cosine similarity â†’ top-k results.  
- Shows similarity score (0â€“1).

#### **4. Technical Details**
- Use `requests` to call local API.  
- Cache embeddings per chunk (avoid recomputation).  
- Normalize vectors for fair cosine comparison.

#### **5. Why Local Embeddings Matter**
- **Privacy**: sensitive docs never leave your machine.  
- **Cost**: $0 per query.  
- **Control**: swap models without rewriting code.

#### **6. When Semantic Wins**
- Paraphrase matching (â€œvehicleâ€ â†’ â€œcarâ€)  
- Conceptual queries (â€œHow do I reduce emissions?â€)  
- No exact keyword match needed

#### **7. Closing**
> â€œYou donâ€™t need the cloud to understand meaning. You just need the right modelâ€”and your own CPU.â€

---

### **Post 7: Hybrid Search â€” The Best of Both Worlds**  
**Subtitle**: *Tab 6: Combining BM25 and semantic for robust retrieval*

#### **1. The Hybrid Promise**
- BM25: great for keywords.  
- Semantic: great for meaning.  
- **Together**: cover more query types, reduce failure modes.

#### **2. Introducing Tab 6: Hybrid Search**
- Select chunk files â†’ choose query â†’ set **k** and **Î± (alpha)**.  
- Î± = weight for BM25 (0 = semantic only, 1 = BM25 only).  
- Real-time results with **combined score**.

#### **3. How Scores Are Combined**
- Normalize BM25 scores to [0,1].  
- Semantic scores already in [0,1].  
- Final score = `Î± * bm25_norm + (1 - Î±) * semantic_score`.

#### **4. The Magic of Tuning Alpha**
- Show examples:  
  - Î±=0.7 â†’ favors keyword precision (legal queries)  
  - Î±=0.3 â†’ favors semantic recall (exploratory queries)  
- Let users **experiment live**â€”this is pedagogy through UI.

#### **5. Why Hybrid Is the RAG Sweet Spot**
- More robust to query variation.  
- Less dependent on perfect embeddings or perfect keywords.  
- **Empirical truth**: hybrid often beats pure semantic in benchmarks.

#### **6. The Full Workflow Recap**
1. PDF â†’ Markdown (Tab 1)  
2. Chunk manually (Tab 2)  
3. Inspect (Tab 3)  
4. Retrieve with BM25 (Tab 4)  
5. Retrieve with semantic (Tab 5)  
6. Combine both (Tab 6)  
â†’ **End-to-end, human-in-the-loop RAG**

#### **7. Closing Vision**
> â€œThe future of RAG isnâ€™t bigger modelsâ€”itâ€™s smarter data, built by humans who care.  
> And now, you have the toolkit to build it.â€

---

### **Bonus: Series CTA (Call to Action)**
- **GitHub repo**: link to full Gradio app (modular, with instructions).  
- **Challenge**: â€œChunk one document this week. Where did your instinct beat the algorithm?â€  
- **Community**: invite readers to share their `_chunks.json` files or chunking strategies.

---

Let me know if you'd like me to **draft any of these posts in full**, generate **Gradio code for Tabs 4â€“6**, or create **diagrams/visuals** to accompany the series!


# POSTS

#### still to be included the code for articles 2 and 3 and 4ss

Absolutely. Below are **fully written drafts** of **Posts 1, 2, and 3** of your Substack seriesâ€”each approximately 2000 words, with a clear narrative arc, technical depth, and practical insights. Theyâ€™re written in an engaging, conversational yet authoritative tone, suitable for technical practitioners, RAG engineers, and AI product builders.

---

## **Post 1: The Forgotten Foundation of RAG**  
### *Why your chunks matter more than your embedding model*

A few months ago, I spent over $200 running LLM-powered â€œchunk optimizationâ€ experiments. I fed the same PDF into GraphRAG, LightRAG, and a few LangChain auto-splitters. I tweaked prompts, adjusted overlap windows, even tried letting an LLM rewrite my documents into â€œRAG-friendlyâ€ summaries.

The result? Worse retrieval. Confusing answers. And a lot of wasted tokens.

Then I did something radical: I opened the PDF in a text editor, read it carefully, and **drew the chunk boundaries myself**.

Suddenly, retrieval worked. Not â€œkind of.â€ Not â€œwith caveats.â€ It justâ€¦ worked.

Thatâ€™s when I realized: **the biggest bottleneck in RAG isnâ€™t your modelâ€”itâ€™s your data preparation**.

### The Illusion of Model-Centric RAG

We live in an age where every RAG tutorial starts the same way:  
> â€œJust plug in your embedding model and vector database!â€

But this skips the most critical step: **making your data retrieval-ready**.

Current frameworks, for all their innovation, often treat documents as inert bags of tokens:
- **LangChainâ€™s default splitters** cut at fixed token countsâ€”regardless of whether youâ€™re mid-sentence, mid-table, or mid-legal-definition.
- **GraphRAG** (Microsoftâ€™s impressive framework) uses LLMs to extract entities and relationships, then builds a knowledge graph. Powerful? Yes. But it requires **dozens of LLM calls per document**, costs scale non-linearly, and the output is opaqueâ€”even to its creators.
- **LightRAG** improves on this by using smaller LLM calls to summarize local context, but it still **relies on stochastic reasoning** to decide what belongs together.

All these approaches share a hidden assumption: *that an LLM is the best judge of semantic boundaries*.

But hereâ€™s a secret: **itâ€™s not**.

An LLM doesnâ€™t know that a footnote belongs to the paragraph above.  
It doesnâ€™t know that a financial disclaimer applies to the entire section.  
It doesnâ€™t know that a case study illustrates the principle introduced two pages earlier.

**You do.**

And thatâ€™s the core idea behind this series: **the best person to chunk your document is youâ€”the domain expert**.

### Why Chunking Is the Hidden Labor of RAG

Retrieval-Augmented Generation (RAG) has three stages:
1. **Retrieval**: find relevant context  
2. **Augmentation**: inject that context into a prompt  
3. **Generation**: produce an answer  

But retrieval only works if the **retrievable units**â€”your chunksâ€”are meaningful.

Think of chunking like **indexing a book**. A good index doesnâ€™t cut entries mid-sentence. It groups concepts logically: â€œClimate policy â†’ EU regulations, p. 45â€“48.â€ A bad index? â€œClimate, p. 45; policy, p. 46; regulations, p. 47â€¦â€â€”useless.

Yet in RAG, we often hand this critical task to algorithms that have **no understanding of your domain**, your audience, or your intent.

The result? Chunks that are:
- **Too small**: missing necessary context (â€œUSB-Câ€ without compatibility notes)
- **Too large**: drowning the LLM in irrelevant text
- **Semantically broken**: half a definition + half an unrelated example

And then we blame the embedding model when retrieval fails.

### Introducing a Human-Centered Alternative

What if we flipped the script?

Instead of asking, *â€œHow can we make LLMs better at chunking?â€*  
We ask: *â€œHow can we make it easy for humans to chunk well?â€*

Thatâ€™s the philosophy behind the toolkit weâ€™ll build in this series: a **modular, transparent, and iterative** approach to RAG data preparation.

Our principles:
- **Transparent**: you see every chunk before itâ€™s used.
- **Auditable**: every decision is recorded in structured JSON.
- **Efficient**: no wasted LLM calls, no hidden costs.
- **Modular**: each step is a standalone tool you can use today.

Over the next seven posts, weâ€™ll build a Gradio application with six tabs:
1. Convert PDFs to clean Markdown  
2. Manually set chunk boundaries  
3. Inspect your chunks  
4. Retrieve with BM25 (yes, itâ€™s still great!)  
5. Add local semantic search  
6. Combine both for hybrid retrieval  

By the end, youâ€™ll have a full RAG prep and retrieval suiteâ€”**that starts with you, not a model**.

### The Bigger Picture: RAG as a Craft, Not Just an API

Weâ€™ve been sold a vision of RAG as plug-and-play: drop in your data, hit â€œembed,â€ and get smart answers.

But real-world RAG is more like **bookbinding than button-pushing**. It requires care, judgment, and iteration.

The good news? **You donâ€™t need an LLM to do this well.**  
You need:
- Clean text (not PDF garbage)  
- Structured delimiters (so you can mark boundaries)  
- Token awareness (so chunks fit in LLM context)  
- A way to inspect and refine  

Thatâ€™s it.

And as weâ€™ll see in the next post, the first stepâ€”converting PDFs into something usableâ€”is where most RAG pipelines fail before they even begin.

> **Great RAG doesnâ€™t start with a prompt. It starts with a paragraph break.**

*Next week: Why your PDFs are RAG poisonâ€”and how Markdown is the antidote.*

---

## **Post 2: From PDF Chaos to Markdown Clarity**  
### *Tab 1: Why your RAG pipeline begins with structured text*

If youâ€™ve ever tried to build a RAG system from a PDF, you know the pain.

You upload a 50-page white paper. You run it through a PDF parser. And what comes out?

> â€œFigur e 3: Marke t T rend s â€¦ â€¦ â€¦ â€¦ â€¦ â€¦ â€¦ â€¦ â€¦ â€¦ â€¦ â€¦ â€¦ â€¦ â€¦ â€¦ â€¦ â€¦ T ot al Rev enue inc reased by 12% in Q3 .â€

Columns are scrambled. Tables become word salads. Headers and footers leak into body text. Page numbers float like ghosts in the middle of sentences.

**PDFs werenâ€™t designed for machines. They were designed for printers.**

And yet, we keep feeding them into RAG pipelines as if theyâ€™re clean data.

Theyâ€™re not. Theyâ€™re **layout artifacts**â€”beautiful for humans, toxic for retrieval.

### The Markdown Revelation

The solution isnâ€™t fancier parsing. Itâ€™s **using the right intermediate format**.

Enter **Markdown**.

Why Markdown?
- Itâ€™s **human-readable**: you can open it in any text editor and understand it.
- Itâ€™s **structured**: headers (`#`), lists (`-`), code blocks (```) preserve document hierarchy.
- Itâ€™s **editable**: unlike HTML or XML, you donâ€™t need special tools to tweak it.
- And cruciallyâ€”it supports **non-rendering comments** like `<!-- CHUNK -->`, perfect for marking boundaries without affecting display.

Most importantly: **Markdown is retrieval-ready**.  
When your text has clean structure, deciding where to chunk becomes intuitive.

### Introducing Tab 1: PDF â†’ Markdown Converter

In our Gradio app, **Tab 1** solves the PDF problem in one click.

Built with **Docling**â€”an open-source, high-fidelity document converter from IBMâ€”this tab:
1. Accepts a PDF upload  
2. Converts it to clean Markdown (preserving tables, lists, headings)  
3. Saves the output as a `.md` file in your working directory  

No cloud APIs. No hidden costs. Just pure, local conversion.

Hereâ€™s what changes:
- A **scrambled table** becomes a proper Markdown table with aligned columns.  
- **Multi-column layouts** are reflowed into logical reading order.  
- **Headers/footers/page numbers** are removed or isolated.  
- **Bold/italic** formatting is preserved as `**text**` or `*text*`.

The result? A document thatâ€™s **not just readableâ€”but chunkable**.

### Why This Matters for RAG

Letâ€™s be clear: **you cannot build a good RAG system on top of PDF noise**.

Consider this financial snippet:
> â€œTotal Revenue: $1.2B (see footnote 3).â€

In a raw PDF extract, â€œfootnote 3â€ might appear 20 lines laterâ€”or not at all.  
In Doclingâ€™s Markdown output, itâ€™s cleanly attached:
```markdown
Total Revenue: $1.2B[^3]

[^3]: Excludes one-time asset sales.
```

Now, when you chunk, you can **keep the footnote with the figure**â€”because the structure makes it obvious.

Compare that to recursive character splitting, which might give you:
- Chunk 1: â€œTotal Revenue: $1.2B (see footnâ€
- Chunk 2: â€œote 3).â€

Useless.

### A Real-World Example

I tested this on a 30-page EU regulatory document.

- **PyPDF2 output**: 42 instances of â€œArt. 5â€ scattered randomly, tables turned into â€œ| â€” | â€” |â€, page numbers in the middle of sentences.
- **Docling â†’ Markdown**: clean article headings (`## Article 5`), intact definitions, tables as proper grids.

Then I asked: *â€œWhat are the reporting requirements under Article 5?â€*

With PyPDF2 chunks: irrelevant fragments about Article 3, page numbers, and table borders.  
With Markdown chunks: the full Article 5 sectionâ€”retrieved in one go.

**The difference wasnâ€™t the retriever. It was the input.**

### Try It Yourself

The code for Tab 1 is minimal:
```python
from docling.document_converter import DocumentConverter

def convert_pdf_to_md(pdf_file):
    converter = DocumentConverter()
    result = converter.convert(pdf_file.name)
    md_content = result.document.export_to_markdown()
    # Save to disk
```

And the Gradio UI? Just a file upload and a button.

But the impact is massive: **youâ€™ve just detoxed your data**.

> Youâ€™re not converting documents. Youâ€™re preparing them for conversation.

*Next week: Why youâ€”not an algorithmâ€”are the best person to draw chunk boundaries.*

---

## **Post 3: You Are the Best Chunker**  
### *Tab 2: Why domain expertsâ€”not algorithmsâ€”should draw the lines*

Last week, we turned PDF chaos into Markdown clarity.

Now comes the most important step: **deciding where to split that Markdown into chunks**.

Most tutorials hand this task to algorithms:
- â€œUse RecursiveCharacterTextSplitter with chunk_size=512!â€
- â€œLet an LLM summarize each paragraph!â€

But hereâ€™s the truth: **no algorithm knows your document like you do**.

You know that:
- The footnote belongs with the paragraph above.  
- The case study illustrates the principle on the prior page.  
- The legal clause ends after the semicolonâ€”not at token 512.

An LLM guesses. **You know.**

### The Problem with Auto-Chunking

Auto-splitters operate on **tokens, not meaning**.

They ask: â€œHave we hit 512 tokens yet?â€  
Not: â€œDoes this sentence complete a thought?â€

The result? Chunks like:
> â€œThe system shall comply with GDPR Article 17(1), which states that data subjects have the right toâ€¦â€

â€¦cut off mid-sentence because the token counter ran out.

Or worseâ€”chunks that mix unrelated topics because they happened to fit in 512 tokens.

LLM-based chunking tries to fix this by asking the model: â€œSummarize this section.â€ But now youâ€™ve:
- Spent $0.02 per chunk  
- Introduced hallucination risk  
- Lost the original wording (which might contain key terms for BM25)

And for what? A â€œcleanâ€ summary that still might not align with **your retrieval needs**.

### Introducing Tab 2: The Chunk Editor

In our Gradio app, **Tab 2** puts you in control.

Hereâ€™s how it works:
1. **Select a Markdown file** (from Tab 1 output)  
2. **Edit the text** in a large textbox  
3. **Insert `<!-- CHUNK -->`** anywhere you want a boundary  
4. Click â€œProcess Chunksâ€ â†’ get a preview + structured JSON  

You see the **original document** in a preview pane (so you donâ€™t lose context).  
You edit a **copy** (so the source stays pristine).  
And when you process, you get:
- Token count per chunk (using `tiktoken`)  
- A preview of each chunkâ€™s first 100 characters  
- A JSON file with full metadata: `{filename, chunk_index, token_count, text}`

This isnâ€™t automation. **Itâ€™s augmentation.**  
The tool handles the bookkeeping; you handle the judgment.

### A Chunking Case Study

I tested this on a product manual for a medical device.

- **Auto-splitter result**:  
  Chunk 12: â€œTo sterilize the device, useâ€¦â€  
  Chunk 13: â€œâ€¦autoclave at 121Â°C for 15 minutes. Warning: Do not use chemical sterilants.â€

The warningâ€”critical for safetyâ€”was separated from the instruction.

- **Manual chunk (mine)**:  
  Kept the full instruction + warning together, because **I knew** they were a single safety unit.

When a nurse queried: *â€œHow do I sterilize the device?â€*, the manual chunk returned the complete protocolâ€”including the warning.

The auto-splitter? Only the first half.  
**Thatâ€™s not just inaccurateâ€”itâ€™s dangerous.**

### Why This Scales

â€œBut I have thousands of documents!â€ you say.

Fair. Manual chunking doesnâ€™t scale to millions of pages.

But it **does scale to your core documents**â€”the ones that matter most:
- Legal contracts  
- Product manuals  
- Compliance guidelines  
- Research reports  

Chunk these **once, well**, and reuse them forever across:
- BM25 retrieval  
- Semantic search  
- Hybrid systems  
- Fine-tuning datasets  

And for bulk documents? Use your **manually chunked examples as training data** for a custom splitter.

### The Philosophy: Intentional Over Automatic

We fetishize automation, but **not everything should be automated**.

Chunking is a **semantic decision**, not a syntactic one.  
It requires understanding of:
- Domain logic  
- User intent  
- Risk (what happens if this is split wrong?)

Thatâ€™s why the best RAG systems Iâ€™ve seen arenâ€™t the ones with the fanciest embeddingsâ€”theyâ€™re the ones where **a human carefully prepared the data**.

> An LLM sees tokens. You see meaning. Trust your judgment.

*Next week: How to inspect your chunksâ€”because if you canâ€™t see your data, you donâ€™t own your RAG system.*

---

Let me know when you're ready for **Posts 4â€“7**, or if you'd like **Gradio code for Tabs 4â€“6 (BM25, semantic, hybrid)** to accompany the later posts!

Absolutely! Below are **fully written drafts of Posts 4 through 7**, each ~2000 words, continuing the narrative and technical depth of your Substack series. At the end, youâ€™ll find **complete, integrated Gradio code for Tabs 4â€“6** (BM25, semantic, hybrid search) that **seamlessly extends your existing app** (Tabs 1â€“3) with no breaking changes.

---

## **Post 4: Inspect, Donâ€™t Assume**  
### *Tab 3: Making your chunks visibleâ€”and trustworthy*

Last week, I asked you to take control of chunk boundariesâ€”because you, not an algorithm, understand your documentâ€™s meaning.

But control without feedback is just guesswork.

Imagine editing a photo in the dark. You adjust contrast, crop, sharpenâ€”but you canâ€™t see the result until itâ€™s printed. Thatâ€™s how most RAG pipelines operate: chunks are created, embedded, and retrieved **without ever being inspected**.

And when retrieval fails, we blame the model, the embedding, or the queryâ€”not the invisible, broken chunks at the root of the problem.

### The Silent Failure of RAG

RAG doesnâ€™t crash when chunks are bad. It **lies politely**.

Ask: *â€œWhatâ€™s the penalty for late submission?â€*  
Get an answer that cites the wrong clauseâ€”because your chunk combined two unrelated sections.

Thereâ€™s no error. No warning. Just a confidently wrong response.

This is why **observability is non-negotiable** in RAG. And observability starts with **seeing your data**.

### Introducing Tab 3: The Chunk Inspector

In our Gradio app, **Tab 3** makes the invisible visible.

Hereâ€™s how it works:
- A dropdown lists all `_chunks.json` files (output from Tab 2).  
- Select one â†’ the full JSON structure appears in an expandable viewer.  
- Every chunk shows:  
  - Source filename  
  - Chunk index  
  - Token count  
  - Full text  

No more black boxes. Just raw, auditable data.

This isnâ€™t just debuggingâ€”itâ€™s **data stewardship**. You can:
- Spot chunks that are too long (>1024 tokens) and risk truncation.  
- Find empty or near-empty chunks (often page numbers or artifacts).  
- Verify that boundaries align with semantic breaks (e.g., no â€œcontâ€¦â€ at the end).  
- Confirm token distribution is balanced (not 50 tokens here, 2000 there).

### A Real Inspection Session

I loaded a `_chunks.json` from a legal contract Iâ€™d manually chunked.

- **Chunk 7**: 1,248 tokens â†’ too big for most LLM contexts.  
  â†’ I went back to Tab 2, split it after the subsection header.  
- **Chunk 12**: â€œPage 8 of 24â€ â†’ noise.  
  â†’ Deleted it in the editor.  
- **Chunk 15**: ended with â€œincluding but not limited toâ€”â€  
  â†’ Clearly cut mid-list. Fixed by moving `<!-- CHUNK -->` two lines down.

Without Tab 3, Iâ€™d never have known.

### The Iterative Refinement Loop

This is where **true RAG craftsmanship** happens:
1. **Edit** in Tab 2  
2. **Process** â†’ generate JSON  
3. **Inspect** in Tab 3  
4. **Refine** â†’ repeat  

Each cycle improves chunk quality. And because your chunks are saved as structured JSON, every version is **reproducible and shareable**.

Imagine onboarding a new team member:  
> â€œHereâ€™s how we chunked the compliance manual. See how we kept definitions with examples? Thatâ€™s our standard.â€

This is **institutional knowledge made explicit**.

### Beyond Debugging: Your Chunks as Documentation

Your `_chunks.json` files arenâ€™t just inputs for retrievalâ€”theyâ€™re **living documentation** of your domain logic.

- Why is this footnote attached to that paragraph? The chunk shows it.  
- Why did we separate these two clauses? The boundary is visible.  
- How many tokens does a typical section use? The metadata tells you.

In a world of black-box AI, this transparency is radicalâ€”and powerful.

### The Road Ahead

Now that your chunks are **clean, intentional, and visible**, we can finally talk about retrieval.

But not the fancy kind. Not yet.

First, we return to a classic: **BM25**â€”the unsung hero of keyword search.

> If you canâ€™t see your data, you donâ€™t own your RAG system.

*Next week: Why BM25 isnâ€™t obsoleteâ€”itâ€™s your secret weapon.*

---

## **Post 5: BM25 Is Not Deadâ€”Itâ€™s Underrated**  
### *Tab 4: Fast, free, and shockingly effective keyword search*

Letâ€™s talk about BM25.

If youâ€™re under 30 and work in AI, you might think BM25 is a relicâ€”a keyword-search dinosaur from the pre-embedding era.

But hereâ€™s a secret: **BM25 powers Google, Elasticsearch, and half the search engines you use daily**.

And with clean, human-chunked data? Itâ€™s **scary good**.

### Why BM25 Shines with Good Chunks

BM25 (Best Match 25) is a ranking function that scores documents based on:
- **Term frequency**: how often query words appear  
- **Inverse document frequency**: how rare those words are across the corpus  

Itâ€™s simple. Itâ€™s fast. Itâ€™s **deterministic**.

But it fails spectacularly on noisy data:
- Garbled PDF text â†’ low term frequency  
- Overly large chunks â†’ diluted relevance  
- Broken boundaries â†’ keywords isolated from context  

**Your manually chunked Markdown? Perfect BM25 fuel.**

Example:  
Query: *â€œclimate risk disclosure requirementsâ€*  

In a financial report chunked by an auto-splitter:  
- â€œclimateâ€ in chunk A  
- â€œriskâ€ in chunk B  
- â€œdisclosure requirementsâ€ in chunk C  
â†’ BM25 finds nothing relevant.

In your manually chunked version:  
- All terms appear together in one coherent chunk about ESG reporting  
â†’ BM25 scores it highly. Retrieval succeeds.

### Introducing Tab 4: BM25 Search

In our Gradio app, **Tab 4** brings BM25 to life.

Features:
- **Multi-file selection**: choose any number of `_chunks.json` files  
- **Query input**: type your question  
- **Top-k slider**: how many results to return  
- **Results display**: ranked list with BM25 scores and chunk text  

Under the hood:  
- We use `rank_bm25` (pure Python, no heavy dependencies)  
- Chunks are tokenized using simple whitespace splitting (BM25 doesnâ€™t need fancy tokenization)  
- Scores are computed on-the-flyâ€”no pre-indexing needed for small datasets

### Code Philosophy: Transparency Over Magic

The BM25 implementation is deliberately **visible and minimal**:
```python
from rank_bm25 import BM25Okapi

corpus = [chunk["text"].split() for chunk in all_chunks]
bm25 = BM25Okapi(corpus)
query_tokens = query.split()
scores = bm25.get_scores(query_tokens)
```

No black-box APIs. No hidden state. Just math you can understand.

This aligns with our core principle: **RAG should be inspectable at every layer**.

### When BM25 Wins

BM25 dominates when:
- Queries contain **exact phrases** (â€œSection 4.2(a)â€)  
- Youâ€™re searching for **named entities** (laws, products, codes)  
- You need **zero-latency, zero-cost** retrieval  
- You value **explainability** (â€œIt matched because â€˜GDPRâ€™ appears 3 timesâ€)

In enterprise settingsâ€”legal, finance, complianceâ€”this covers 60%+ of queries.

### Limitations (and Why Weâ€™ll Add Semantic Next)

BM25 wonâ€™t help if:
- You ask *â€œHow do I reduce carbon emissions?â€* but the document says *â€œlower greenhouse gas outputâ€*  
- Your query uses synonyms the document doesnâ€™t  

But thatâ€™s not a flawâ€”itâ€™s a **scope**. BM25 handles lexical matching. Something else should handle semantic matching.

And that â€œsomething elseâ€ doesnâ€™t need to cost you $0.01 per query.

> BM25 doesnâ€™t need to be smart. It just needs clean dataâ€”and youâ€™ve already built that.

*Next week: How to run semantic search on your laptopâ€”with no cloud, no cost, and full privacy.*

---

## **Post 6: Semantic Search on Your Laptop**  
### *Tab 5: Private, local embeddings with llama.cpp*

If BM25 is the reliable workhorse of retrieval, semantic search is the intuitive artist.

It understands that *â€œcarâ€* and *â€œautomobileâ€* mean the same thing.  
That *â€œHow do I reset my password?â€* is related to *â€œForgot login credentials.â€*

But most tutorials assume youâ€™ll use OpenAIâ€™s embedding APIâ€”or pay for Pinecone, Weaviate, or Azure.

What if you could run **state-of-the-art embeddings locally**, for free, with full privacy?

You can. And itâ€™s easier than you think.

### Enter llama.cpp: Your Embedding Server

[llama.cpp](https://github.com/ggerganov/llama.cpp) isnâ€™t just for LLMsâ€”it supports **embedding models** too.

With a single command, you can run models like:
- `nomic-embed-text` (Apache 2.0, 8192 context)  
- `bge-m3` (via `llama-box`)  
- `all-MiniLM-L6-v2` (via GGUF conversion)  

And expose them via an **OpenAI-compatible API** at `http://localhost:8080/v1`.

No internet required after download. No API keys. No per-query fees.

Just your CPU (or GPU, if you have one) and a few GB of RAM.

### Introducing Tab 5: Semantic Search

In our Gradio app, **Tab 5** connects to your local embedding server.

Workflow:
1. Select one or more `_chunks.json` files  
2. Enter a query  
3. Get top-k results ranked by **cosine similarity**  

Under the hood:
- On first load, chunks are embedded via `POST /v1/embeddings`  
- Embeddings are **cached** (so you donâ€™t recompute them)  
- Query is embedded â†’ compared to all chunk vectors â†’ ranked  

Crucially: **no data leaves your machine**.

### Why Local Embeddings Matter

- **Privacy**: sensitive contracts, medical records, internal docs stay local  
- **Cost**: $0 per query vs. $0.0001â€“$0.001 on cloud APIs  
- **Control**: swap models without changing your app logic  
- **Offline use**: perfect for air-gapped environments  

And performance? On a modern laptop, embedding 100 chunks takes <10 seconds. Retrieval is instant.

### Technical Notes: Making It Work

To use Tab 5:
1. Install `llama-box` or `llm-rs`  
2. Download a GGUF embedding model (e.g., `nomic-embed-text.Q4_K_M.gguf`)  
3. Run:  
   ```bash
   llama-box --embeddings --model nomic-embed-text.Q4_K_M.gguf --port 8080
   ```
4. Open Tab 5 in your Gradio app

The app assumes the server is running at `http://localhost:8080`â€”but you can make this configurable later.

### When Semantic Search Wins

Use semantic search when:
- Queries are **paraphrased** or **conceptual**  
- You need **recall over precision** (find all related ideas)  
- Your documents use **domain-specific jargon** that BM25 wonâ€™t match  

But remember: **garbage in, garbage out**.  
If your chunks are poorly bounded, even the best embedding model will fail.

Thatâ€™s why we started with **you**â€”the human chunker.

> You donâ€™t need the cloud to understand meaning. You just need the right modelâ€”and your own CPU.

*Next week: How to combine BM25 and semantic search into a hybrid system thatâ€™s greater than the sum of its parts.*

---

## **Post 7: Hybrid Search â€” The Best of Both Worlds**  
### *Tab 6: Combining BM25 and semantic for robust retrieval*

Weâ€™ve arrived at the final piece of the puzzle.

- **BM25**: fast, free, perfect for exact matches  
- **Semantic search**: intuitive, flexible, great for concepts  

But real-world queries arenâ€™t purely lexical or purely semantic.  
Theyâ€™re **both**.

> â€œWhatâ€™s the deadline for GDPR compliance reports?â€  
â†’ Needs exact match on â€œGDPR compliance reportsâ€ (BM25)  
â†’ But â€œdeadlineâ€ might appear as â€œdue dateâ€ or â€œsubmission windowâ€ (semantic)

This is where **hybrid search** shines.

### Introducing Tab 6: Hybrid Search

In our Gradio app, **Tab 6** fuses BM25 and semantic scores into a single ranking.

Features:
- Select multiple `_chunks.json` files  
- Enter a query  
- Adjust **Î± (alpha)**: BM25 weight (0 = semantic only, 1 = BM25 only)  
- Get top-k results with **combined score**

The magic happens in the scoring:
```python
bm25_norm = (bm25_score - min_bm25) / (max_bm25 - min_bm25)  # [0,1]
semantic_score = cosine_sim  # already [0,1]
final_score = alpha * bm25_norm + (1 - alpha) * semantic_score
```

This simple weighted sum **covers more query types** and **reduces failure modes**.

### Why Hybrid Works

- **BM25 anchors precision**: ensures exact terms arenâ€™t drowned out  
- **Semantic adds recall**: catches paraphrases and related concepts  
- **Tunable balance**: adapt to your domain (legal = high Î±, research = low Î±)

In benchmarks, hybrid consistently outperforms pure semantic or pure lexical search.

### A Live Demo

Try this query in Tab 6:  
> â€œHow to sterilize the deviceâ€

With Î± = 0.7 (BM25-favored):  
- Top result: exact phrase â€œsterilize the deviceâ€ from manual  

With Î± = 0.3 (semantic-favored):  
- Top result: â€œautoclave instructionsâ€ (no exact match, but high semantic similarity)

With Î± = 0.5:  
- Both appear in top 2 â†’ user gets full context

**Thatâ€™s robustness.**

### The Full Workflow, Realized

Letâ€™s recap your end-to-end RAG prep and retrieval suite:
1. **Tab 1**: PDF â†’ clean Markdown  
2. **Tab 2**: Manually set chunk boundaries  
3. **Tab 3**: Inspect and refine chunks  
4. **Tab 4**: Retrieve with BM25  
5. **Tab 5**: Retrieve with local semantic search  
6. **Tab 6**: Hybrid retrieval with tunable fusion  

Every tab is **modular and standalone**. You could stop at Tab 3 and already have a better RAG foundation than most teams.

### The Bigger Vision

This series wasnâ€™t just about building a tool.  
It was about **reclaiming agency in the RAG pipeline**.

Weâ€™ve shown that:
- Great retrieval starts with **thoughtful data**, not bigger models  
- **Humans** are the best judges of semantic boundaries  
- **Transparency** beats black-box automation  
- **Local, private, free** tools can outperform cloud APIs  

And you now have a **fully working, extendable Gradio app** to prove it.

> The future of RAG isnâ€™t bigger modelsâ€”itâ€™s smarter data, built by humans who care.  
> And now, you have the toolkit to build it.

---

## âœ… **Integrated Gradio Code for Tabs 4â€“6**

Add the following to your existing app **after Tab 3 and before `demo.launch()`**.  
It assumes:
- `rank_bm25` is installed (`pip install rank-bm25`)
- A local embedding server runs at `http://localhost:8080/v1` (see Post 6 instructions)
- Chunks are stored as `_chunks.json` files in `OUTPUT_DIR`

```python
import numpy as np
import requests
from rank_bm25 import BM25Okapi
from pathlib import Path
import hashlib

# Cache for embeddings: {chunk_hash: embedding_vector}
embedding_cache = {}

def get_embedding(text: str, model: str = "nomic-embed-text") -> list:
    """Get embedding from local llama.cpp server."""
    cache_key = hashlib.md5(text.encode()).hexdigest()
    if cache_key in embedding_cache:
        return embedding_cache[cache_key]

    try:
        response = requests.post(
            "http://localhost:8080/v1/embeddings",
            json={"input": text, "model": model},
            timeout=30
        )
        response.raise_for_status()
        embedding = response.json()["data"][0]["embedding"]
        embedding_cache[cache_key] = embedding
        return embedding
    except Exception as e:
        print(f"Embedding error: {e}")
        # Fallback: zero vector (will rank last)
        return [0.0] * 768  # adjust size if your model differs

def cosine_similarity(a, b):
    a, b = np.array(a), np.array(b)
    return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))

# ----------------------------
# Tab 4: BM25 Search
# ----------------------------
def list_chunk_files_for_search():
    return [f.name for f in OUTPUT_DIR.glob("*_chunks.json")]

def bm25_search(chunk_filenames, query, top_k=5):
    if not chunk_filenames or not query:
        return "âš ï¸ Select files and enter a query.", []

    all_chunks = []
    for filename in chunk_filenames:
        try:
            with open(OUTPUT_DIR / filename, "r", encoding="utf-8") as f:
                chunks = json.load(f)
                all_chunks.extend(chunks)
        except Exception as e:
            return f"âŒ Error loading {filename}: {e}", []

    if not all_chunks:
        return "âŒ No chunks found.", []

    # Prepare corpus for BM25
    corpus = [chunk["text"].split() for chunk in all_chunks]
    bm25 = BM25Okapi(corpus)
    query_tokens = query.split()
    scores = bm25.get_scores(query_tokens)

    # Get top-k
    top_indices = np.argsort(scores)[::-1][:top_k]
    results = []
    output_lines = [f"âœ… Top {min(top_k, len(scores))} BM25 Results:\n"]
    for i, idx in enumerate(top_indices):
        chunk = all_chunks[idx]
        score = scores[idx]
        preview = chunk["text"][:200] + ("..." if len(chunk["text"]) > 200 else "")
        line = f"[{i+1}] Score: {score:.2f} | {chunk['filename']} (Chunk {chunk['chunk_index']})\n{preview}\n"
        output_lines.append(line)
        results.append(chunk)

    return "\n".join(output_lines), results

# ----------------------------
# Tab 5: Semantic Search
# ----------------------------
def semantic_search(chunk_filenames, query, top_k=5):
    if not chunk_filenames or not query:
        return "âš ï¸ Select files and enter a query.", []

    all_chunks = []
    for filename in chunk_filenames:
        try:
            with open(OUTPUT_DIR / filename, "r", encoding="utf-8") as f:
                chunks = json.load(f)
                all_chunks.extend(chunks)
        except Exception as e:
            return f"âŒ Error loading {filename}: {e}", []

    if not all_chunks:
        return "âŒ No chunks found.", []

    # Embed query
    query_emb = get_embedding(query)

    # Compute similarities
    similarities = []
    for chunk in all_chunks:
        chunk_emb = get_embedding(chunk["text"])
        sim = cosine_similarity(query_emb, chunk_emb)
        similarities.append(sim)

    # Get top-k
    top_indices = np.argsort(similarities)[::-1][:top_k]
    output_lines = [f"âœ… Top {min(top_k, len(similarities))} Semantic Results:\n"]
    results = []
    for i, idx in enumerate(top_indices):
        chunk = all_chunks[idx]
        sim = similarities[idx]
        preview = chunk["text"][:200] + ("..." if len(chunk["text"]) > 200 else "")
        line = f"[{i+1}] Similarity: {sim:.4f} | {chunk['filename']} (Chunk {chunk['chunk_index']})\n{preview}\n"
        output_lines.append(line)
        results.append(chunk)

    return "\n".join(output_lines), results

# ----------------------------
# Tab 6: Hybrid Search
# ----------------------------
def hybrid_search(chunk_filenames, query, top_k=5, alpha=0.5):
    if not chunk_filenames or not query:
        return "âš ï¸ Select files and enter a query.", []

    all_chunks = []
    for filename in chunk_filenames:
        try:
            with open(OUTPUT_DIR / filename, "r", encoding="utf-8") as f:
                chunks = json.load(f)
                all_chunks.extend(chunks)
        except Exception as e:
            return f"âŒ Error loading {filename}: {e}", []

    if not all_chunks:
        return "âŒ No chunks found.", []

    # BM25 scores
    corpus = [chunk["text"].split() for chunk in all_chunks]
    bm25 = BM25Okapi(corpus)
    query_tokens = query.split()
    bm25_scores = bm25.get_scores(query_tokens)
    min_bm25, max_bm25 = min(bm25_scores), max(bm25_scores)
    if min_bm25 == max_bm25:
        bm25_norm = [0.5] * len(bm25_scores)  # avoid div by zero
    else:
        bm25_norm = [(s - min_bm25) / (max_bm25 - min_bm25) for s in bm25_scores]

    # Semantic scores
    query_emb = get_embedding(query)
    semantic_scores = []
    for chunk in all_chunks:
        chunk_emb = get_embedding(chunk["text"])
        sim = cosine_similarity(query_emb, chunk_emb)
        semantic_scores.append(sim)

    # Hybrid score
    hybrid_scores = [
        alpha * bm25_n + (1 - alpha) * sem
        for bm25_n, sem in zip(bm25_norm, semantic_scores)
    ]

    # Get top-k
    top_indices = np.argsort(hybrid_scores)[::-1][:top_k]
    output_lines = [f"âœ… Top {min(top_k, len(hybrid_scores))} Hybrid Results (Î±={alpha:.2f}):\n"]
    results = []
    for i, idx in enumerate(top_indices):
        chunk = all_chunks[idx]
        score = hybrid_scores[idx]
        preview = chunk["text"][:200] + ("..." if len(chunk["text"]) > 200 else "")
        line = f"[{i+1}] Hybrid Score: {score:.4f} | {chunk['filename']} (Chunk {chunk['chunk_index']})\n{preview}\n"
        output_lines.append(line)
        results.append(chunk)

    return "\n".join(output_lines), results


# ----------------------------
# Add Tabs 4-6 to Gradio App
# ----------------------------
with gr.Blocks(title="PDF to Markdown & Chunk Editor") as demo:
    # ... existing Tabs 1-3 ...

    with gr.Tabs():
        # ... Tabs 1-3 as before ...

        # ---------------- Tab 4: BM25 Search ----------------
        with gr.Tab("ğŸ” BM25 Search"):
            with gr.Row():
                bm25_file_dropdown = gr.Dropdown(
                    choices=list_chunk_files_for_search(),
                    label="Select Chunk JSON Files",
                    multiselect=True,
                    interactive=True
                )
                bm25_refresh_btn = gr.Button("ğŸ”„ Refresh List")
            bm25_query = gr.Textbox(label="Query", placeholder="Enter your search query...")
            bm25_top_k = gr.Slider(1, 20, value=5, step=1, label="Top K Results")
            bm25_search_btn = gr.Button("ğŸ” Search with BM25")
            bm25_output = gr.Textbox(label="BM25 Results", lines=15)

            bm25_refresh_btn.click(
                fn=list_chunk_files_for_search,
                outputs=bm25_file_dropdown
            )
            bm25_search_btn.click(
                fn=bm25_search,
                inputs=[bm25_file_dropdown, bm25_query, bm25_top_k],
                outputs=bm25_output
            )

        # ---------------- Tab 5: Semantic Search ----------------
        with gr.Tab("ğŸ§  Semantic Search"):
            with gr.Row():
                sem_file_dropdown = gr.Dropdown(
                    choices=list_chunk_files_for_search(),
                    label="Select Chunk JSON Files",
                    multiselect=True,
                    interactive=True
                )
                sem_refresh_btn = gr.Button("ğŸ”„ Refresh List")
            sem_query = gr.Textbox(label="Query", placeholder="Enter your search query...")
            sem_top_k = gr.Slider(1, 20, value=5, step=1, label="Top K Results")
            sem_search_btn = gr.Button("ğŸ§  Search with Embeddings")
            sem_output = gr.Textbox(label="Semantic Search Results", lines=15)

            sem_refresh_btn.click(
                fn=list_chunk_files_for_search,
                outputs=sem_file_dropdown
            )
            sem_search_btn.click(
                fn=semantic_search,
                inputs=[sem_file_dropdown, sem_query, sem_top_k],
                outputs=sem_output
            )

        # ---------------- Tab 6: Hybrid Search ----------------
        with gr.Tab("âš¡ Hybrid Search"):
            with gr.Row():
                hybrid_file_dropdown = gr.Dropdown(
                    choices=list_chunk_files_for_search(),
                    label="Select Chunk JSON Files",
                    multiselect=True,
                    interactive=True
                )
                hybrid_refresh_btn = gr.Button("ğŸ”„ Refresh List")
            hybrid_query = gr.Textbox(label="Query", placeholder="Enter your search query...")
            with gr.Row():
                hybrid_top_k = gr.Slider(1, 20, value=5, step=1, label="Top K Results")
                hybrid_alpha = gr.Slider(0.0, 1.0, value=0.5, step=0.05, label="BM25 Weight (Î±)")
            hybrid_search_btn = gr.Button("âš¡ Search with Hybrid")
            hybrid_output = gr.Textbox(label="Hybrid Search Results", lines=15)

            hybrid_refresh_btn.click(
                fn=list_chunk_files_for_search,
                outputs=hybrid_file_dropdown
            )
            hybrid_search_btn.click(
                fn=hybrid_search,
                inputs=[hybrid_file_dropdown, hybrid_query, hybrid_top_k, hybrid_alpha],
                outputs=hybrid_output
            )

# ... rest of demo.launch() ...
```


# FINAL SETUP UNTIL TAB 6

```python
import os
import json
from pathlib import Path
from docling.document_converter import DocumentConverter
import gradio as gr
import tiktoken

# Constants
CHUNK_DELIMITER = "<!-- CHUNK -->"
OUTPUT_DIR = Path.cwd()
ENCODING_NAME = "cl100k_base"  # Standard for GPT-3.5/4; change if needed

# Initialize tokenizer once
tokenizer = tiktoken.get_encoding(ENCODING_NAME)

def count_tokens(text: str) -> int:
    return len(tokenizer.encode(text))

# Ensure output dir exists
OUTPUT_DIR.mkdir(exist_ok=True)

def list_chunk_files():
    return [f.name for f in OUTPUT_DIR.glob("*_chunks.json")]  

# ----------------------------
# Tab 1: PDF to Markdown
# ----------------------------
def convert_pdf_to_md(pdf_file):
    if pdf_file is None:
        return "âŒ No file uploaded."
    
    try:
        converter = DocumentConverter()
        result = converter.convert(pdf_file.name)
        md_content = result.document.export_to_markdown()

        # Derive output filename
        md_filename = Path(pdf_file.name).with_suffix('.md').name
        md_path = OUTPUT_DIR / md_filename

        with open(md_path, "w", encoding="utf-8") as f:
            f.write(md_content)

        return f"âœ… Successfully converted and saved as `{md_filename}`"
    except Exception as e:
        return f"âŒ Error: {str(e)}"


# ----------------------------
# Tab 2: Markdown Chunking
# ----------------------------
def list_markdown_files():
    return [f.name for f in OUTPUT_DIR.glob("*.md")]

def load_markdown_file(filename):
    if not filename:
        return "", ""
    try:
        with open(OUTPUT_DIR / filename, "r", encoding="utf-8") as f:
            content = f.read()
        return content, content
    except Exception as e:
        return f"Error loading file: {e}", ""

def process_chunks(md_text, filename):
    if not md_text or not filename:
        return "âš ï¸ No content or filename.", None

    raw_chunks = md_text.split(CHUNK_DELIMITER)
    chunks = []
    total_tokens = 0
    preview_lines = []

    for i, chunk_text in enumerate(raw_chunks):
        chunk_text = chunk_text.strip()
        if not chunk_text:
            continue

        token_count = count_tokens(chunk_text)
        total_tokens += token_count

        chunk_meta = {
            "filename": filename,
            "chunk_index": i,
            "token_count": token_count,
            "text": chunk_text
        }
        chunks.append(chunk_meta)

        # Preview line: show first 100 chars + token count
        preview = f"[Chunk {i}] ({token_count} tokens)\n{chunk_text[:100]}{'...' if len(chunk_text) > 100 else ''}"
        preview_lines.append(preview)

    # Build preview display
    display_text = (
        f"âœ… Total chunks: {len(chunks)} | Total tokens: {total_tokens}\n\n"
        + ("\n" + "="*60 + "\n").join(preview_lines)
    )

    # Save as JSON
    json_path = OUTPUT_DIR / f"{Path(filename).stem}_chunks.json"
    with open(json_path, "w", encoding="utf-8") as f:
        json.dump(chunks, f, indent=2, ensure_ascii=False)

    return display_text, str(json_path)

import numpy as np
import requests
from rank_bm25 import BM25Okapi
from pathlib import Path
import hashlib

# Cache for embeddings: {chunk_hash: embedding_vector}
embedding_cache = {}

def get_embedding(text: str, model: str = "nomic-embed-text") -> list:
    """Get embedding from local llama.cpp server."""
    cache_key = hashlib.md5(text.encode()).hexdigest()
    if cache_key in embedding_cache:
        return embedding_cache[cache_key]

    try:
        response = requests.post(
            "http://localhost:8080/v1/embeddings",
            json={"input": text, "model": model},
            timeout=30
        )
        response.raise_for_status()
        embedding = response.json()["data"][0]["embedding"]
        embedding_cache[cache_key] = embedding
        return embedding
    except Exception as e:
        print(f"Embedding error: {e}")
        # Fallback: zero vector (will rank last)
        return [0.0] * 384  # adjust size if your model differs

def cosine_similarity(a, b):
    a = np.array(a)
    b = np.array(b)
    
    norm_a = np.linalg.norm(a)
    norm_b = np.linalg.norm(b)
    
    if norm_a == 0 or norm_b == 0:
        return 0.0  # Zero vector has no direction â†’ similarity = 0
    
    return np.dot(a, b) / (norm_a * norm_b)

# ----------------------------
# Tab 4: BM25 Search
# ----------------------------
def list_chunk_files_for_search():
    return [f.name for f in OUTPUT_DIR.glob("*_chunks.json")]

def bm25_search(chunk_filenames, query, top_k=5):
    if not chunk_filenames or not query:
        return []

    all_chunks = []
    for filename in chunk_filenames:
        try:
            with open(OUTPUT_DIR / filename, "r", encoding="utf-8") as f:
                chunks = json.load(f)
                all_chunks.extend(chunks)
        except Exception as e:
            return [{"error": f"Failed to load {filename}: {str(e)}"}]

    if not all_chunks:
        return []

    corpus = [chunk["text"].split() for chunk in all_chunks]
    bm25 = BM25Okapi(corpus)
    query_tokens = query.split()
    scores = bm25.get_scores(query_tokens)

    # Get top-k indices
    top_indices = np.argsort(scores)[::-1][:top_k]
    
    results = []
    for idx in top_indices:
        chunk = all_chunks[idx].copy()  # don't mutate original
        chunk["_score_bm25"] = float(scores[idx])  # add score
        results.append(chunk)
    
    return results



# ----------------------------
# Tab 5: Semantic Search
# ----------------------------
def semantic_search(chunk_filenames, query, top_k=5):
    if not chunk_filenames or not query:
        return []

    all_chunks = []
    for filename in chunk_filenames:
        try:
            with open(OUTPUT_DIR / filename, "r", encoding="utf-8") as f:
                chunks = json.load(f)
                all_chunks.extend(chunks)
        except Exception as e:
            return [{"error": f"Failed to load {filename}: {str(e)}"}]

    if not all_chunks:
        return []

    query_emb = get_embedding(query)
    results = []
    similarities = []

    for chunk in all_chunks:
        chunk_emb = get_embedding(chunk["text"])
        sim = float(cosine_similarity(query_emb, chunk_emb))
        similarities.append(sim)

    top_indices = np.argsort(similarities)[::-1][:top_k]
    for idx in top_indices:
        chunk = all_chunks[idx].copy()
        chunk["_score_semantic"] = similarities[idx]
        results.append(chunk)

    return results


# ----------------------------
# Tab 6: Hybrid Search
# ----------------------------

def hybrid_search(chunk_filenames, query, top_k=5, alpha=0.5):
    if not chunk_filenames or not query:
        return []

    all_chunks = []
    for filename in chunk_filenames:
        try:
            with open(OUTPUT_DIR / filename, "r", encoding="utf-8") as f:
                chunks = json.load(f)
                all_chunks.extend(chunks)
        except Exception as e:
            return [{"error": f"Failed to load {filename}: {str(e)}"}]

    if not all_chunks:
        return []

    # BM25 scores
    corpus = [chunk["text"].split() for chunk in all_chunks]
    bm25 = BM25Okapi(corpus)
    query_tokens = query.split()
    bm25_scores = bm25.get_scores(query_tokens)
    min_b, max_b = min(bm25_scores), max(bm25_scores)
    bm25_norm = [0.5] * len(bm25_scores) if min_b == max_b else [(s - min_b) / (max_b - min_b) for s in bm25_scores]

    # Semantic scores
    query_emb = get_embedding(query)
    semantic_scores = []
    for chunk in all_chunks:
        chunk_emb = get_embedding(chunk["text"])
        sim = float(cosine_similarity(query_emb, chunk_emb))
        semantic_scores.append(sim)

    # Hybrid score
    hybrid_scores = [alpha * b + (1 - alpha) * s for b, s in zip(bm25_norm, semantic_scores)]

    top_indices = np.argsort(hybrid_scores)[::-1][:top_k]
    results = []
    for idx in top_indices:
        chunk = all_chunks[idx].copy()
        chunk["_score_hybrid"] = hybrid_scores[idx]
        chunk["_bm25_norm"] = bm25_norm[idx]
        chunk["_semantic"] = semantic_scores[idx]
        results.append(chunk)

    return results

# ----------------------------
# Gradio Interface
# ----------------------------
with gr.Blocks(title="PDF to Markdown & Chunk Editor") as demo:
    gr.Markdown("# ğŸ“„ PDF to Markdown Converter & Chunk Editor")

    with gr.Tabs():
        # ---------------- Tab 1 ----------------
        with gr.Tab("ğŸ“„ Convert PDF to Markdown"):
            pdf_input = gr.File(label="Upload PDF", file_types=[".pdf"])
            convert_btn = gr.Button("Convert to Markdown")
            convert_output = gr.Textbox(label="Result", interactive=False)

            convert_btn.click(
                fn=convert_pdf_to_md,
                inputs=pdf_input,
                outputs=convert_output
            )

        # ---------------- Tab 2 ----------------
        with gr.Tab("âœ‚ï¸ Chunk Editor"):
            with gr.Row():
                md_file_dropdown = gr.Dropdown(
                    choices=list_markdown_files(),
                    label="Select Markdown File",
                    interactive=True
                )
                refresh_btn = gr.Button("ğŸ”„ Refresh List")

            with gr.Row():
                original_md_preview = gr.Markdown(label="Original Preview")
                editable_md = gr.Textbox(
                    label="Editable Markdown (Insert chunk breaks with <!-- CHUNK -->)",
                    lines=20,
                    max_lines=30
                )

            selected_filename = gr.State("")  # to track current file

            def on_file_select(filename):
                content, _ = load_markdown_file(filename)
                return content, content, filename

            md_file_dropdown.change(
                fn=on_file_select,
                inputs=md_file_dropdown,
                outputs=[editable_md, original_md_preview, selected_filename]
            )

            refresh_btn.click(
                fn=lambda: gr.update(choices=list_markdown_files()),
                inputs=None,
                outputs=md_file_dropdown
            )

            process_btn = gr.Button("âœ‚ï¸ Process Chunks")
            chunks_output = gr.Textbox(label="Processed Chunks (Preview)", lines=10)
            download_json = gr.File(label="Download Chunks as JSON", visible=True)

            process_btn.click(
                fn=process_chunks,
                inputs=[editable_md, selected_filename],
                outputs=[chunks_output, download_json]
            )
        # ---------------- Tab 3: Explore Chunk Files ----------------
        with gr.Tab("ğŸ“Š Explore Chunk Files"):
            with gr.Row():       
                chunk_file_dropdown = gr.Dropdown(
                    choices=list_chunk_files(),
                    label="Select Chunk JSON File",
                    interactive=True,
                    allow_custom_value=True
                )
                refresh_chunk_btn = gr.Button("ğŸ”„ Refresh List")

            chunk_json_viewer = gr.JSON(label="Chunks (click to expand)")

            def load_chunks_as_json(filename):
                if isinstance(filename, list):
                    filename = filename[0] if filename else ""
                if not filename or not isinstance(filename, str):
                    return {}
                try:
                    with open(OUTPUT_DIR / filename, "r", encoding="utf-8") as f:
                        return json.load(f)
                except Exception as e:
                    return {"error": str(e)}

            # Remove the demo.load() line!

            refresh_chunk_btn.click(
                fn=list_chunk_files,
                outputs=chunk_file_dropdown
            )

            chunk_file_dropdown.change(
                fn=load_chunks_as_json,
                inputs=chunk_file_dropdown,
                outputs=chunk_json_viewer
            )

        # ---------------- Tab 4: BM25 Search ----------------
        with gr.Tab("ğŸ” BM25 Search"):
            with gr.Row():
                bm25_file_dropdown = gr.Dropdown(
                    choices=list_chunk_files_for_search(),
                    label="Select Chunk JSON Files",
                    multiselect=True,
                    interactive=True
                )
                bm25_refresh_btn = gr.Button("ğŸ”„ Refresh List")
            bm25_query = gr.Textbox(label="Query", placeholder="Enter your search query...")
            bm25_top_k = gr.Slider(1, 20, value=5, step=1, label="Top K Results")
            bm25_search_btn = gr.Button("ğŸ” Search with BM25")
            bm25_output = gr.JSON(label="BM25 Results")
            bm25_search_btn.click(
                fn=bm25_search,
                inputs=[bm25_file_dropdown, bm25_query, bm25_top_k],
                outputs=bm25_output
            )

            bm25_refresh_btn.click(
                fn=list_chunk_files_for_search,
                outputs=bm25_file_dropdown
            )


        # ---------------- Tab 5: Semantic Search ----------------
        with gr.Tab("ğŸ§  Semantic Search"):
            with gr.Row():
                sem_file_dropdown = gr.Dropdown(
                    choices=list_chunk_files_for_search(),
                    label="Select Chunk JSON Files",
                    multiselect=True,
                    interactive=True
                )
                sem_refresh_btn = gr.Button("ğŸ”„ Refresh List")
            sem_query = gr.Textbox(label="Query", placeholder="Enter your search query...")
            sem_top_k = gr.Slider(1, 20, value=5, step=1, label="Top K Results")
            sem_search_btn = gr.Button("ğŸ§  Search with Embeddings")
            sem_output = gr.JSON(label="Semantic Search Results")
            sem_search_btn.click(
                fn=semantic_search,
                inputs=[sem_file_dropdown, sem_query, sem_top_k],
                outputs=sem_output
            )

            sem_refresh_btn.click(
                fn=list_chunk_files_for_search,
                outputs=sem_file_dropdown
            )


        # ---------------- Tab 6: Hybrid Search ----------------
        with gr.Tab("âš¡ Hybrid Search"):
            with gr.Row():
                hybrid_file_dropdown = gr.Dropdown(
                    choices=list_chunk_files_for_search(),
                    label="Select Chunk JSON Files",
                    multiselect=True,
                    interactive=True
                )
                hybrid_refresh_btn = gr.Button("ğŸ”„ Refresh List")
            hybrid_query = gr.Textbox(label="Query", placeholder="Enter your search query...")
            with gr.Row():
                hybrid_top_k = gr.Slider(1, 20, value=5, step=1, label="Top K Results")
                hybrid_alpha = gr.Slider(0.0, 1.0, value=0.5, step=0.05, label="BM25 Weight (Î±)")
            hybrid_search_btn = gr.Button("âš¡ Search with Hybrid")
            hybrid_output = gr.JSON(label="Hybrid Search Results")
            hybrid_search_btn.click(
                fn=hybrid_search,
                inputs=[hybrid_file_dropdown, hybrid_query, hybrid_top_k, hybrid_alpha],
                outputs=hybrid_output
            )

            hybrid_refresh_btn.click(
                fn=list_chunk_files_for_search,
                outputs=hybrid_file_dropdown
            )


# Launch
if __name__ == "__main__":
    demo.launch()
```

---

### ğŸ”§ Setup Notes:
1. **Install dependencies**:
   ```bash
   pip install rank-bm25 numpy requests
   ```
2. **Run embedding server** (example with `nomic-embed-text`):
   ```bash
   # Download model from Hugging Face (GGUF format)
   # Then:
   llama-box --embeddings --model nomic-embed-text.Q4_K_M.gguf --port 8080
   ```
3. **Adjust embedding dimension** in fallback if your model isnâ€™t 768-dim.

---

You now have a **complete, production-ready RAG preparation and retrieval suite**â€”all local, transparent, and human-centered.

Let me know if you'd like help with **deployment**, **embedding model recommendations**, or **visuals/diagrams** for your Substack!

Absolutely! Visuals and diagrams are **critical** for your Substack seriesâ€”they make abstract RAG concepts tangible, showcase your Gradio appâ€™s workflow, and help readers follow along with your modular philosophy.

Below are **7 custom visual concepts** (one per post), each with:
- A **clear purpose**
- A **description you can give to a designer** (or generate with AI tools like Excalidraw, Mermaid, or Diagrams.net)
- A **suggested placement** in the post
- Optional **Mermaid/ASCII code** for quick rendering

---

### **Visual 1: The RAG Data Funnel (Post 1)**
**Purpose**: Show how most RAG pipelines start with messy data â†’ failed retrieval, while your approach starts with clean, human-prepared chunks.

**Description**:  
A two-column flowchart:
- **Left (Broken RAG)**:  
  `PDF` â†’ `Garbled Text` â†’ `Auto-Chunked (mid-sentence)` â†’ `Bad Retrieval` â†’ `Wrong Answer`  
  (Use red X icons, jagged arrows)
- **Right (Your RAG)**:  
  `PDF` â†’ `Clean Markdown` â†’ `Human-Chunked (meaningful)` â†’ `Good Retrieval` â†’ `Accurate Answer`  
  (Use green checkmarks, smooth arrows)

**Placement**: After the â€œThe Illusion of Model-Centric RAGâ€ section.

**Mermaid Code**:
```mermaid
flowchart LR
    subgraph Broken[â€œTypical RAG (Fails)â€]
        A[PDF] --> B[Garbled Text]
        B --> C[Auto-Chunked\n(mid-sentence)]
        C --> D[Bad Retrieval]
        D --> E[Wrong Answer]
    end

    subgraph Good[â€œYour RAG (Works)â€]
        F[PDF] --> G[Clean Markdown]
        G --> H[Human-Chunked\n(meaningful)]
        H --> I[Good Retrieval]
        I --> J[Accurate Answer]
    end

    style Broken fill:#ffe6e6,stroke:#c00
    style Good fill:#e6f7ee,stroke:#090
```

---

### **Visual 2: PDF vs. Markdown (Post 2)**
**Purpose**: Contrast raw PDF extraction (chaotic) vs. Docling â†’ Markdown (structured).

**Description**:  
Side-by-side text blocks:
- **Left (PDF Extract)**:  
  ```
  Figur e 3 : Marke t T rend s â€¦ â€¦ â€¦ 
  T ot al Rev enue: $1 .2B
  Page 8
  ```
  (Use broken lines, ellipses, misaligned text)
- **Right (Markdown)**:
  ```markdown
  ## Market Trends
  
  Total Revenue: $1.2B[^1]
  
  [^1]: Fiscal year 2024.
  ```
  (Use clean syntax highlighting)

**Placement**: Right after â€œThe Markdown Revelationâ€ section.

**Tip**: Add a small **â€œDoclingâ€ logo** or icon between them with a â€œâ†’â€ arrow.

---

### **Visual 3: Chunk Boundary Decision (Post 3)**
**Purpose**: Show how a human correctly groups related content vs. an auto-splitter breaking meaning.

**Description**:  
A document snippet with **two chunking options**:
- **Auto-Splitter**:  
  `Chunk 12: â€œTo sterilize the device, useâ€¦â€`  
  `Chunk 13: â€œâ€¦autoclave at 121Â°C. Warning: Do not use chemical sterilants.â€`  
  (Add a red â€œâš ï¸â€ next to Chunk 13)
- **Human Chunk**:  
  `Chunk X: â€œTo sterilize the device, use autoclave at 121Â°C for 15 minutes. Warning: Do not use chemical sterilants.â€`  
  (Add a green â€œâœ…â€)

**Placement**: In the â€œA Chunking Case Studyâ€ section.

**ASCII Version**:
```
[Auto-Splitter]          [Human Chunk]
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ To sterilize    â”‚      â”‚ To sterilize the device, use â”‚
â”‚ the device, use â”‚  âš ï¸  â”‚ autoclave at 121Â°C for 15    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚ minutes. Warning: Do not use â”‚
                         â”‚ chemical sterilants.        â”‚  âœ…
                         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

### **Visual 4: Chunk Inspector UI (Post 4)**
**Purpose**: Show what Tab 3 looks likeâ€”making chunks visible.

**Description**:  
A **screenshot-style wireframe** of Tab 3:
- Dropdown: â€œSelect Chunk JSON Fileâ€ (with example: `compliance_chunks.json`)
- JSON viewer showing:
  ```json
  [
    {
      "filename": "compliance.md",
      "chunk_index": 5,
      "token_count": 328,
      "text": "All submissions must be filed by..."
    }
  ]
  ```
- Callout: â€œSee token count, source, and full text!â€

**Placement**: When introducing Tab 3.

**Tip**: If you donâ€™t have a real screenshot yet, use **Gradioâ€™s default JSON component style** (dark background, collapsible arrows).

---

### **Visual 5: BM25 vs. Semantic (Post 5 & 6)**
**Purpose**: Contrast retrieval results from BM25 (exact match) vs. Semantic (conceptual match).

**Description**:  
Two result cards for the same query: *â€œHow to reset password?â€*
- **BM25 Result**:  
  Title: `user_guide.md (Chunk 12)`  
  Snippet: `â€œTo reset your password, go to Settings > Account > Reset Password.â€`  
  Score: `BM25: 24.7`  
  (Icon: ğŸ”)
- **Semantic Result**:  
  Title: `faq.md (Chunk 3)`  
  Snippet: `â€œForgot your login? Use the â€˜Forgot Passwordâ€™ link on the login page.â€`  
  Score: `Similarity: 0.89`  
  (Icon: ğŸ§ )

**Placement**: In Post 5 (BM25 strengths) and Post 6 (semantic strengths).

---

### **Visual 6: Hybrid Search Slider (Post 7)**
**Purpose**: Show how the Î± (alpha) slider changes results.

**Description**:  
A **live-demo mockup** of Tab 6:
- Query: `â€œsterilize deviceâ€`
- Slider: `BM25 Weight (Î±): [0.0 â€”â—â€” 1.0]` with label â€œ0.5â€
- Results list showing two items:
  1. Exact match (high BM25, medium semantic)
  2. Conceptual match (medium BM25, high semantic)
- Annotation: â€œAdjust Î± to balance precision vs. recallâ€

**Placement**: In the â€œA Live Demoâ€ section of Post 7.

---

### **Visual 7: Full App Architecture (Series Recap)**
**Purpose**: Show the complete 6-tab workflow as a modular pipeline.

**Description**:  
A circular or linear diagram with 6 labeled nodes:
1. **PDF â†’ MD** (ğŸ“„)  
2. **Chunk Editor** (âœ‚ï¸)  
3. **Inspector** (ğŸ”)  
4. **BM25** (ğŸ”‘)  
5. **Semantic** (ğŸ§ )  
6. **Hybrid** (âš¡)  

Arrows show flow from 1â†’2â†’3, then 3â†’4,5,6.  
Add caption: â€œEach tab is a standalone tool. Build your RAG pipeline step by step.â€

**Placement**: At the end of Post 7 (The Full Workflow, Realized).

**Mermaid Code**:
```mermaid
flowchart LR
    A[ğŸ“„ PDF â†’ Markdown] --> B[âœ‚ï¸ Chunk Editor]
    B --> C[ğŸ” Inspector]
    C --> D[ğŸ”‘ BM25 Search]
    C --> E[ğŸ§  Semantic Search]
    D --> F[âš¡ Hybrid Search]
    E --> F
```

---

### ğŸ› ï¸ **How to Create These**
1. **Free Tools**:
   - [**Excalidraw**](https://excalidraw.com/): Hand-drawn style, perfect for tech blogs.
   - [**Mermaid Live Editor**](https://mermaid.live/): For flowcharts/diagrams (embed as SVG).
   - [**Diagrams.net**](https://app.diagrams.net/): Professional UML-style diagrams.

2. **AI Image Generators** (for conceptual art):
   - Use Midjourney/DALLÂ·E with prompt:  
     *â€œMinimalist tech diagram showing PDF to Markdown conversion, clean lines, blue and green colors, flat designâ€*

3. **Gradio Screenshots**:
   - Run your app â†’ take real screenshots for Tabs 3â€“6 â†’ annotate with arrows/callouts in **Preview (Mac)** or **Snagit**.

---

### âœ… **Final Tip: Visual Consistency**
- Use the **same color scheme** across all diagrams:
  - **Red**: problems / broken flow  
  - **Green**: your solution / success  
  - **Blue**: neutral / tools  
- Use **consistent icons** (ğŸ“„, âœ‚ï¸, ğŸ”, etc.) from your Gradio tab titles.

---

Let me know if youâ€™d like:
- Ready-to-copy **Excalidraw JSON files**
- **High-res PNG templates**
- **Animated GIFs** of the Gradio app in action

These visuals will make your series **stand out** as both deeply technical and beautifully clear.



---


Perfect! You already have a robust **6-tab RAG preparation and retrieval toolkit**. Now you want to add **Tab 7: Full RAG with LLM**, using:

- **Retrieval**: results from your **Hybrid Search** (Tab 6)  
- **LLM**: served via **llama.cpp** with **OpenAI-compatible API** at `http://localhost:8081`

Below is a **complete, integrated Tab 7** that:
1. Reuses your `hybrid_search` function for retrieval  
2. Sends retrieved chunks + query to your LLM  
3. Displays the **final RAG answer** + **retrieved context** (for transparency)  
4. Is **fully compatible** with your existing code and Gradio 3.x

---

### âœ… **Step 1: Add LLM Completion Function**

Add this **above your Gradio UI** (after the hybrid search function):

```python
def rag_complete(chunk_filenames, query, top_k=5, alpha=0.5, system_prompt=None):
    """
    Perform full RAG: retrieve with hybrid search, then generate answer with LLM.
    Returns: {"answer": "...", "retrieved_chunks": [...]}
    """
    if not chunk_filenames or not query:
        return {"error": "âš ï¸ Select files and enter a query."}

    # Step 1: Retrieve top chunks using hybrid search
    retrieved_chunks = hybrid_search(chunk_filenames, query, top_k=top_k, alpha=alpha)
    if not retrieved_chunks or "error" in str(retrieved_chunks):
        return {"error": "âŒ Retrieval failed.", "retrieved_chunks": retrieved_chunks}

    # Step 2: Build context from retrieved chunks
    context = "\n\n".join([
        f"[Source: {chunk.get('filename', 'unknown')}]\n{chunk['text']}"
        for chunk in retrieved_chunks
    ])

    # Step 3: Build messages for OpenAI-compatible API
    default_system = (
        "You are a helpful assistant that answers questions based ONLY on the provided context. "
        "If the context doesn't contain the answer, say 'I don't know based on the provided documents.'"
    )
    messages = [
        {"role": "system", "content": system_prompt or default_system},
        {"role": "user", "content": f"Context:\n{context}\n\nQuestion: {query}"}
    ]

    # Step 4: Call LLM server at localhost:8081
    try:
        response = requests.post(
            "http://localhost:8081/v1/chat/completions",
            json={
                "model": "bge-small-en-v1.5",  # or your actual model name
                "messages": messages,
                "temperature": 0.3,
                "max_tokens": 500
            },
            timeout=120
        )
        response.raise_for_status()
        answer = response.json()["choices"][0]["message"]["content"].strip()
    except Exception as e:
        return {
            "error": f"LLM server error: {str(e)}",
            "retrieved_chunks": retrieved_chunks
        }

    return {
        "answer": answer,
        "retrieved_chunks": retrieved_chunks
    }
```

> ğŸ” **Note**:  
> - Uses `http://localhost:8081/v1/chat/completions` (standard OpenAI chat endpoint)  
> - Includes **source filenames** in context for traceability  
> - Fails gracefully with error in JSON output

---

### âœ… **Step 2: Add Tab 7 to Your Gradio UI**

Add this **inside your `with gr.Tabs():` block**, after Tab 6:

```python
        # ---------------- Tab 7: Full RAG with LLM ----------------
        with gr.Tab("ğŸ¤– Full RAG (Hybrid + LLM)"):
            with gr.Row():
                rag_file_dropdown = gr.Dropdown(
                    choices=list_chunk_files_for_search(),
                    label="Select Chunk JSON Files",
                    multiselect=True,
                    interactive=True
                )
                rag_refresh_btn = gr.Button("ğŸ”„ Refresh List")
            
            rag_query = gr.Textbox(label="Question", placeholder="Ask a question about your documents...")
            
            with gr.Row():
                rag_top_k = gr.Slider(1, 20, value=5, step=1, label="Top K Chunks")
                rag_alpha = gr.Slider(0.0, 1.0, value=0.5, step=0.05, label="BM25 Weight (Î±)")
            
            rag_system_prompt = gr.Textbox(
                label="System Prompt (optional)",
                placeholder="Customize LLM behavior...",
                lines=2
            )
            
            rag_btn = gr.Button("ğŸ¤– Get Answer")
            rag_answer = gr.Textbox(label="Answer", lines=8, interactive=False)
            rag_context = gr.JSON(label="Retrieved Chunks (for verification)")
            
            # Refresh file list
            rag_refresh_btn.click(
                fn=list_chunk_files_for_search,
                outputs=rag_file_dropdown
            )
            
            # Main RAG function
            rag_btn.click(
                fn=rag_complete,
                inputs=[rag_file_dropdown, rag_query, rag_top_k, rag_alpha, rag_system_prompt],
                outputs=[rag_answer, rag_context]
            )
```

---

### âš™ï¸ **How to Run Your LLM Server**

Make sure your **llama.cpp LLM server** is running at `localhost:8081` with **OpenAI API compatibility**:

```bash
# Example with llama.cpp server (adjust path/model)
./server -m your-chat-model.Q4_K_M.gguf --port 8081 --chat

# Or with llama-box (if available)
llama-box --model your-chat-model.Q4_K_M.gguf --port 8081
```

> ğŸ’¡ **Important**:  
> - This is **separate from your embedding server** (which runs at `:8080`)  
> - Use a **chat-optimized model** (e.g., `Mistral-7B-Instruct`, `Phi-3`, `Llama-3-8B-Instruct`) â€” **not** your embedding model  
> - Your embedding model (`bge-small-en-v1.5`) is **only for Tab 5/6**, not for generation

---

### ğŸŒŸ **Why This Design Works**

1. **Reuses your hybrid retrieval** â†’ no duplicate code  
2. **Shows both answer AND context** â†’ users can verify grounding  
3. **Custom system prompt** â†’ lets users control LLM behavior  
4. **Error-tolerant** â†’ fails gracefully if LLM/retrieval fails  
5. **Transparent** â†’ uses same JSON format as Tabs 3â€“6  

---

### ğŸ“ **Example Workflow in Tab 7**

1. Select `manual_chunks.json`  
2. Ask: *â€œHow do I sterilize the device?â€*  
3. Set Î±=0.7 (BM25-weighted)  
4. Click **Get Answer**  
5. See:  
   - âœ… **Answer**: â€œUse an autoclave at 121Â°C for 15 minutes. Do not use chemical sterilants.â€  
   - ğŸ” **Retrieved Chunks**: Full chunk with source filename and text  

---

### ğŸ”’ **Security Note**
Since you're running everything locally (`localhost`), there are no external calls â€” your documents **never leave your machine**.

---

You now have a **complete, end-to-end, local RAG system**:
- **Tabs 1â€“3**: Prepare & inspect data  
- **Tabs 4â€“6**: Retrieve with control  
- **Tab 7**: Generate grounded answers  

This is the **perfect capstone** for your Substack series! Let me know if you'd like to add **token counting for the LLM prompt** or **streaming output** next.

# FINAL APP called gradio_docling_ALL7TABS.py

### RUN THE MODELS
```bash
.\llama-server.exe -m C:\FABIO-AI\MODELS_Small\Qwen3-0.6B-Q8_0.gguf --port 8081 -c 8192
.\llama-server.exe -m C:\FABIO-AI\MODELS_embeddings\bge-small-en-v1.5_fp16.gguf --port 8080 --embedding
```

### The python code

```python
import os
import json
from pathlib import Path
from docling.document_converter import DocumentConverter
import gradio as gr
import tiktoken
import time
import numpy as np
import requests
from rank_bm25 import BM25Okapi
from pathlib import Path
import hashlib

# MODELS RUNNING
# .\llama-server.exe -m C:\FABIO-AI\MODELS_Small\Qwen3-0.6B-Q8_0.gguf --port 8081 -c 8192
# .\llama-server.exe -m C:\FABIO-AI\MODELS_embeddings\bge-small-en-v1.5_fp16.gguf --port 8080 --embedding

# Constants
CHUNK_DELIMITER = "<!-- CHUNK -->"
OUTPUT_DIR = Path.cwd()
ENCODING_NAME = "cl100k_base"  # Standard for GPT-3.5/4; change if needed

# Initialize tokenizer once
tokenizer = tiktoken.get_encoding(ENCODING_NAME)

def count_tokens(text: str) -> int:
    return len(tokenizer.encode(text))

def count_prompt_tokens(system_prompt: str, context: str, query: str) -> int:
    """Count tokens in the full RAG prompt using tiktoken."""
    full_prompt = f"{system_prompt}\n\nContext:\n{context}\n\nQuestion: {query}"
    return count_tokens(full_prompt)

# Ensure output dir exists
OUTPUT_DIR.mkdir(exist_ok=True)

def list_chunk_files():
    return [f.name for f in OUTPUT_DIR.glob("*_chunks.json")]  

# ----------------------------
# Tab 1: PDF to Markdown
# ----------------------------
def convert_pdf_to_md(pdf_file):
    if pdf_file is None:
        return "âŒ No file uploaded."
    
    try:
        converter = DocumentConverter()
        result = converter.convert(pdf_file.name)
        md_content = result.document.export_to_markdown()

        # Derive output filename
        md_filename = Path(pdf_file.name).with_suffix('.md').name
        md_path = OUTPUT_DIR / md_filename

        with open(md_path, "w", encoding="utf-8") as f:
            f.write(md_content)

        return f"âœ… Successfully converted and saved as `{md_filename}`"
    except Exception as e:
        return f"âŒ Error: {str(e)}"


# ----------------------------
# Tab 2: Markdown Chunking
# ----------------------------
def list_markdown_files():
    return [f.name for f in OUTPUT_DIR.glob("*.md")]

def load_markdown_file(filename):
    if not filename:
        return "", ""
    try:
        with open(OUTPUT_DIR / filename, "r", encoding="utf-8") as f:
            content = f.read()
        return content, content
    except Exception as e:
        return f"Error loading file: {e}", ""

def process_chunks(md_text, filename):
    if not md_text or not filename:
        return "âš ï¸ No content or filename.", None

    raw_chunks = md_text.split(CHUNK_DELIMITER)
    chunks = []
    total_tokens = 0
    preview_lines = []

    for i, chunk_text in enumerate(raw_chunks):
        chunk_text = chunk_text.strip()
        if not chunk_text:
            continue

        token_count = count_tokens(chunk_text)
        total_tokens += token_count

        chunk_meta = {
            "filename": filename,
            "chunk_index": i,
            "token_count": token_count,
            "text": chunk_text
        }
        chunks.append(chunk_meta)

        # Preview line: show first 100 chars + token count
        preview = f"[Chunk {i}] ({token_count} tokens)\n{chunk_text[:100]}{'...' if len(chunk_text) > 100 else ''}"
        preview_lines.append(preview)

    # Build preview display
    display_text = (
        f"âœ… Total chunks: {len(chunks)} | Total tokens: {total_tokens}\n\n"
        + ("\n" + "="*60 + "\n").join(preview_lines)
    )

    # Save as JSON
    json_path = OUTPUT_DIR / f"{Path(filename).stem}_chunks.json"
    with open(json_path, "w", encoding="utf-8") as f:
        json.dump(chunks, f, indent=2, ensure_ascii=False)

    return display_text, str(json_path)



# Cache for embeddings: {chunk_hash: embedding_vector}
embedding_cache = {}

def get_embedding(text: str, model: str = "nomic-embed-text") -> list:
    """Get embedding from local llama.cpp server."""
    cache_key = hashlib.md5(text.encode()).hexdigest()
    if cache_key in embedding_cache:
        return embedding_cache[cache_key]

    try:
        response = requests.post(
            "http://localhost:8080/v1/embeddings",
            json={"input": text, "model": model},
            timeout=30
        )
        response.raise_for_status()
        embedding = response.json()["data"][0]["embedding"]
        embedding_cache[cache_key] = embedding
        return embedding
    except Exception as e:
        print(f"Embedding error: {e}")
        # Fallback: zero vector (will rank last)
        return [0.0] * 384  # adjust size if your model differs

def cosine_similarity(a, b):
    a = np.array(a)
    b = np.array(b)
    
    norm_a = np.linalg.norm(a)
    norm_b = np.linalg.norm(b)
    
    if norm_a == 0 or norm_b == 0:
        return 0.0  # Zero vector has no direction â†’ similarity = 0
    
    return np.dot(a, b) / (norm_a * norm_b)

# ----------------------------
# Tab 4: BM25 Search
# ----------------------------
def list_chunk_files_for_search():
    return [f.name for f in OUTPUT_DIR.glob("*_chunks.json")]

def bm25_search(chunk_filenames, query, top_k=5):
    if not chunk_filenames or not query:
        return []

    all_chunks = []
    for filename in chunk_filenames:
        try:
            with open(OUTPUT_DIR / filename, "r", encoding="utf-8") as f:
                chunks = json.load(f)
                all_chunks.extend(chunks)
        except Exception as e:
            return [{"error": f"Failed to load {filename}: {str(e)}"}]

    if not all_chunks:
        return []

    corpus = [chunk["text"].split() for chunk in all_chunks]
    bm25 = BM25Okapi(corpus)
    query_tokens = query.split()
    scores = bm25.get_scores(query_tokens)

    # Get top-k indices
    top_indices = np.argsort(scores)[::-1][:top_k]
    
    results = []
    for idx in top_indices:
        chunk = all_chunks[idx].copy()  # don't mutate original
        chunk["_score_bm25"] = float(scores[idx])  # add score
        results.append(chunk)
    
    return results



# ----------------------------
# Tab 5: Semantic Search
# ----------------------------
def semantic_search(chunk_filenames, query, top_k=5):
    if not chunk_filenames or not query:
        return []

    all_chunks = []
    for filename in chunk_filenames:
        try:
            with open(OUTPUT_DIR / filename, "r", encoding="utf-8") as f:
                chunks = json.load(f)
                all_chunks.extend(chunks)
        except Exception as e:
            return [{"error": f"Failed to load {filename}: {str(e)}"}]

    if not all_chunks:
        return []

    query_emb = get_embedding(query)
    results = []
    similarities = []

    for chunk in all_chunks:
        chunk_emb = get_embedding(chunk["text"])
        sim = float(cosine_similarity(query_emb, chunk_emb))
        similarities.append(sim)

    top_indices = np.argsort(similarities)[::-1][:top_k]
    for idx in top_indices:
        chunk = all_chunks[idx].copy()
        chunk["_score_semantic"] = similarities[idx]
        results.append(chunk)

    return results


# ----------------------------
# Tab 6: Hybrid Search
# ----------------------------

def hybrid_search(chunk_filenames, query, top_k=5, alpha=0.5):
    if not chunk_filenames or not query:
        return []

    all_chunks = []
    for filename in chunk_filenames:
        try:
            with open(OUTPUT_DIR / filename, "r", encoding="utf-8") as f:
                chunks = json.load(f)
                all_chunks.extend(chunks)
        except Exception as e:
            return [{"error": f"Failed to load {filename}: {str(e)}"}]

    if not all_chunks:
        return []

    # BM25 scores
    corpus = [chunk["text"].split() for chunk in all_chunks]
    bm25 = BM25Okapi(corpus)
    query_tokens = query.split()
    bm25_scores = bm25.get_scores(query_tokens)
    min_b, max_b = min(bm25_scores), max(bm25_scores)
    bm25_norm = [0.5] * len(bm25_scores) if min_b == max_b else [(s - min_b) / (max_b - min_b) for s in bm25_scores]

    # Semantic scores
    query_emb = get_embedding(query)
    semantic_scores = []
    for chunk in all_chunks:
        chunk_emb = get_embedding(chunk["text"])
        sim = float(cosine_similarity(query_emb, chunk_emb))
        semantic_scores.append(sim)

    # Hybrid score
    hybrid_scores = [alpha * b + (1 - alpha) * s for b, s in zip(bm25_norm, semantic_scores)]

    top_indices = np.argsort(hybrid_scores)[::-1][:top_k]
    results = []
    for idx in top_indices:
        chunk = all_chunks[idx].copy()
        chunk["_score_hybrid"] = hybrid_scores[idx]
        chunk["_bm25_norm"] = bm25_norm[idx]
        chunk["_semantic"] = semantic_scores[idx]
        results.append(chunk)

    return results

# -------------------------------
# Tab 7: FULL RAG & Hybrid Search
# -------------------------------
def rag_complete_full(chunk_filenames, query, top_k=5, alpha=0.5, system_prompt=None):
    """
    Non-streaming RAG that returns full answer + timing + chunks.
    Fully compatible with Gradio 3.x.
    """
    start_time = time.time()
    
    if not chunk_filenames or not query:
        return "âš ï¸ Please select chunk files and enter a question.", [], "â³ Ready"

    # Simulate "retrieval in progress" by just computing
    retrieved_chunks = hybrid_search(chunk_filenames, query, top_k=top_k, alpha=alpha)
    if not retrieved_chunks or (isinstance(retrieved_chunks, list) and len(retrieved_chunks) > 0 and "error" in retrieved_chunks[0]):
        return "âŒ Retrieval failed. Check your chunk files.", retrieved_chunks, "ğŸš¨ Retrieval error"

    context = "\n\n".join([
        f"[Source: {chunk.get('filename', 'unknown')}]\n{chunk['text']}"
        for chunk in retrieved_chunks
    ])

    default_system = (
        "You are a helpful assistant that answers questions based ONLY on the provided context. "
        "If the context doesn't contain the answer, say 'I don't know based on the provided documents.'"
    )
    sys_msg = system_prompt or default_system

    prompt_tokens = count_prompt_tokens(sys_msg, context, query)
    
    # Call LLM (non-streaming)
    messages = [
        {"role": "system", "content": sys_msg},
        {"role": "user", "content": f"Context:\n{context}\n\nQuestion: {query}"}
    ]

    try:
        response = requests.post(
            "http://localhost:8081/v1/chat/completions",
            json={
                "messages": messages,
                "temperature": 0.3,
                "max_tokens": 500,
                "stream": False  # â† Non-streaming
            },
            timeout=190
        )
        response.raise_for_status()
        answer = response.json()["choices"][0]["message"]["content"].strip()
        
        total_time = time.time() - start_time
        status = f"âœ… Done in {total_time:.1f}s | ğŸ“Š Prompt: {prompt_tokens} tokens"
        return answer, retrieved_chunks, status

    except Exception as e:
        total_time = time.time() - start_time
        status = f"ğŸš¨ Failed after {total_time:.1f}s"
        return f"ğŸš¨ LLM error: {str(e)}", retrieved_chunks, status

# ----------------------------
# Gradio Interface
# ----------------------------
with gr.Blocks(title="PDF to Markdown & Chunk Editor") as demo:
    gr.Markdown("# ğŸ“„ PDF to Markdown Converter & Chunk Editor")

    with gr.Tabs():
        # ---------------- Tab 1 ----------------
        with gr.Tab("ğŸ“„ Convert PDF to Markdown"):
            pdf_input = gr.File(label="Upload PDF", file_types=[".pdf"])
            convert_btn = gr.Button("Convert to Markdown")
            convert_output = gr.Textbox(label="Result", interactive=False)

            convert_btn.click(
                fn=convert_pdf_to_md,
                inputs=pdf_input,
                outputs=convert_output
            )

        # ---------------- Tab 2 ----------------
        with gr.Tab("âœ‚ï¸ Chunk Editor"):
            with gr.Row():
                md_file_dropdown = gr.Dropdown(
                    choices=list_markdown_files(),
                    label="Select Markdown File",
                    interactive=True
                )
                refresh_btn = gr.Button("ğŸ”„ Refresh List")

            with gr.Row():
                original_md_preview = gr.Markdown(label="Original Preview")
                editable_md = gr.Textbox(
                    label="Editable Markdown (Insert chunk breaks with <!-- CHUNK -->)",
                    lines=20,
                    max_lines=30
                )

            selected_filename = gr.State("")  # to track current file

            def on_file_select(filename):
                content, _ = load_markdown_file(filename)
                return content, content, filename

            md_file_dropdown.change(
                fn=on_file_select,
                inputs=md_file_dropdown,
                outputs=[editable_md, original_md_preview, selected_filename]
            )

            refresh_btn.click(
                fn=lambda: gr.update(choices=list_markdown_files()),
                inputs=None,
                outputs=md_file_dropdown
            )

            process_btn = gr.Button("âœ‚ï¸ Process Chunks")
            chunks_output = gr.Textbox(label="Processed Chunks (Preview)", lines=10)
            download_json = gr.File(label="Download Chunks as JSON", visible=True)

            process_btn.click(
                fn=process_chunks,
                inputs=[editable_md, selected_filename],
                outputs=[chunks_output, download_json]
            )
        # ---------------- Tab 3: Explore Chunk Files ----------------
        with gr.Tab("ğŸ“Š Explore Chunk Files"):
            with gr.Row():       
                chunk_file_dropdown = gr.Dropdown(
                    choices=list_chunk_files(),
                    label="Select Chunk JSON File",
                    interactive=True,
                    allow_custom_value=True
                )
                refresh_chunk_btn = gr.Button("ğŸ”„ Refresh List")

            chunk_json_viewer = gr.JSON(label="Chunks (click to expand)")

            def load_chunks_as_json(filename):
                if isinstance(filename, list):
                    filename = filename[0] if filename else ""
                if not filename or not isinstance(filename, str):
                    return {}
                try:
                    with open(OUTPUT_DIR / filename, "r", encoding="utf-8") as f:
                        return json.load(f)
                except Exception as e:
                    return {"error": str(e)}

            # Remove the demo.load() line!

            refresh_chunk_btn.click(
                fn=list_chunk_files,
                outputs=chunk_file_dropdown
            )

            chunk_file_dropdown.change(
                fn=load_chunks_as_json,
                inputs=chunk_file_dropdown,
                outputs=chunk_json_viewer
            )

        # ---------------- Tab 4: BM25 Search ----------------
        with gr.Tab("ğŸ” BM25 Search"):
            with gr.Row():
                bm25_file_dropdown = gr.Dropdown(
                    choices=list_chunk_files_for_search(),
                    label="Select Chunk JSON Files",
                    multiselect=True,
                    interactive=True
                )
                bm25_refresh_btn = gr.Button("ğŸ”„ Refresh List")
            bm25_query = gr.Textbox(label="Query", placeholder="Enter your search query...")
            bm25_top_k = gr.Slider(1, 20, value=5, step=1, label="Top K Results")
            bm25_search_btn = gr.Button("ğŸ” Search with BM25")
            bm25_output = gr.JSON(label="BM25 Results")
            bm25_search_btn.click(
                fn=bm25_search,
                inputs=[bm25_file_dropdown, bm25_query, bm25_top_k],
                outputs=bm25_output
            )

            bm25_refresh_btn.click(
                fn=list_chunk_files_for_search,
                outputs=bm25_file_dropdown
            )


        # ---------------- Tab 5: Semantic Search ----------------
        with gr.Tab("ğŸ§  Semantic Search"):
            with gr.Row():
                sem_file_dropdown = gr.Dropdown(
                    choices=list_chunk_files_for_search(),
                    label="Select Chunk JSON Files",
                    multiselect=True,
                    interactive=True
                )
                sem_refresh_btn = gr.Button("ğŸ”„ Refresh List")
            sem_query = gr.Textbox(label="Query", placeholder="Enter your search query...")
            sem_top_k = gr.Slider(1, 20, value=5, step=1, label="Top K Results")
            sem_search_btn = gr.Button("ğŸ§  Search with Embeddings")
            sem_output = gr.JSON(label="Semantic Search Results")
            sem_search_btn.click(
                fn=semantic_search,
                inputs=[sem_file_dropdown, sem_query, sem_top_k],
                outputs=sem_output
            )

            sem_refresh_btn.click(
                fn=list_chunk_files_for_search,
                outputs=sem_file_dropdown
            )


        # ---------------- Tab 6: Hybrid Search ----------------
        with gr.Tab("âš¡ Hybrid Search"):
            with gr.Row():
                hybrid_file_dropdown = gr.Dropdown(
                    choices=list_chunk_files_for_search(),
                    label="Select Chunk JSON Files",
                    multiselect=True,
                    interactive=True
                )
                hybrid_refresh_btn = gr.Button("ğŸ”„ Refresh List")
            hybrid_query = gr.Textbox(label="Query", placeholder="Enter your search query...")
            with gr.Row():
                hybrid_top_k = gr.Slider(1, 20, value=5, step=1, label="Top K Results")
                hybrid_alpha = gr.Slider(0.0, 1.0, value=0.5, step=0.05, label="BM25 Weight (Î±)")
            hybrid_search_btn = gr.Button("âš¡ Search with Hybrid")
            hybrid_output = gr.JSON(label="Hybrid Search Results")
            hybrid_search_btn.click(
                fn=hybrid_search,
                inputs=[hybrid_file_dropdown, hybrid_query, hybrid_top_k, hybrid_alpha],
                outputs=hybrid_output
            )

            hybrid_refresh_btn.click(
                fn=list_chunk_files_for_search,
                outputs=hybrid_file_dropdown
            )

        # ---------------- Tab 7: Full RAG with LLM (Streaming + Token Count) ----------------
        with gr.Tab("ğŸ¤– Full RAG (Hybrid + LLM)"):
            with gr.Row():
                rag_file_dropdown = gr.Dropdown(
                    choices=list_chunk_files_for_search(),
                    label="Select Chunk JSON Files",
                    multiselect=True,
                    interactive=True
                )
                rag_refresh_btn = gr.Button("ğŸ”„ Refresh List")
            
            rag_query = gr.Textbox(label="Question", placeholder="Ask a question about your documents...")
            
            with gr.Row():
                rag_top_k = gr.Slider(1, 20, value=5, step=1, label="Top K Chunks")
                rag_alpha = gr.Slider(0.0, 1.0, value=0.5, step=0.05, label="BM25 Weight (Î±)")
            
            rag_system_prompt = gr.Textbox(
                label="System Prompt (optional)",
                placeholder="Customize LLM behavior...",
                lines=2
            )
            
            rag_btn = gr.Button("ğŸ¤– Get Answer")
            rag_token_count = gr.Textbox(label="Token Usage", interactive=False)
            rag_answer = gr.Textbox(label="Answer", lines=8, interactive=False)
            rag_context = gr.JSON(label="Retrieved Chunks (for verification)")
            
            # Refresh file list
            rag_refresh_btn.click(
                fn=list_chunk_files_for_search,
                outputs=rag_file_dropdown
            )
            
            # Connect streaming function
            rag_btn.click(
                fn=rag_complete_full,  # â† Non-streaming
                inputs=[rag_file_dropdown, rag_query, rag_top_k, rag_alpha, rag_system_prompt],
                outputs=[rag_answer, rag_context, rag_token_count]
            )

# Launch
if __name__ == "__main__":
    demo.launch()
```